"""
FILE: services/author_analyzer.py
PURPOSE: Comprehensive author analyzer that searches extensively for journalist information
LOCATION: services/author_analyzer.py
"""

import os
import re
import json
import logging
import time
from datetime import datetime
from urllib.parse import quote, urlparse

import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class AuthorAnalyzer:
    """Enhanced author analyzer that searches extensively for journalist information online"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def analyze_authors(self, author_text, domain=None):
        """Analyze multiple authors from byline text"""
        authors = self._parse_authors(author_text)
        results = []
        
        for author_name in authors:
            result = self.analyze_single_author(author_name, domain)
            results.append(result)
        
        return results
    
    def analyze_single_author(self, author_name, domain=None):
        """Analyze a single author with comprehensive web search"""
        logger.info(f"Starting comprehensive analysis for author: {author_name} from domain: {domain}")
        
        # Clean author name
        clean_name = self._clean_author_name(author_name)
        
        # Check cache first (if database is available)
        try:
            from models import db, AuthorCache
            from datetime import timedelta
            
            cached = AuthorCache.query.filter_by(author_name=clean_name).first()
            if cached and not cached.is_expired:
                logger.info(f"Returning cached author data for {clean_name}")
                return cached.lookup_data
        except:
            # Database not available, continue without cache
            pass
        
        # Initialize comprehensive result structure
        result = {
            'name': clean_name,
            'found': False,
            'bio': None,
            'image_url': None,
            'credibility_score': 50,
            'articles_count': 0,
            'professional_info': {
                'current_position': None,
                'outlets': [],
                'years_experience': None,
                'expertise_areas': [],
                'previous_positions': [],
                'education': [],
                'certifications': []
            },
            'awards_recognition': {
                'awards': [],
                'honors': [],
                'fellowships': [],
                'grants': []
            },
            'online_presence': {
                'twitter': None,
                'linkedin': None,
                'personal_website': None,
                'outlet_profile': None,
                'email': None,
                'wikipedia': None,
                'muckrack': None,
                'contently': None
            },
            'verification_status': {
                'verified': False,
                'journalist_verified': False,
                'outlet_staff': False,
                'award_winner': False,
                'published_books': False
            },
            'publications': {
                'books': [],
                'major_stories': [],
                'outlets_contributed': []
            },
            'sources_checked': [],
            'search_depth': 'comprehensive',
            'issues_corrections': False,
            'credibility_explanation': {
                'level': 'Unknown',
                'explanation': 'Limited information available',
                'advice': 'Verify claims through additional sources'
            }
        }
        
        # Comprehensive search strategy
        search_strategies = [
            ('outlet_page', self._check_outlet_author_page),
            ('google_comprehensive', self._comprehensive_google_search),
            ('awards_search', self._search_journalism_awards),
            ('education_search', self._search_education_background),
            ('linkedin_deep', self._deep_linkedin_search),
            ('twitter_analysis', self._analyze_twitter_presence),
            ('wikipedia', self._check_wikipedia),
            ('journalism_databases', self._check_all_journalism_databases),
            ('book_search', self._search_published_books),
            ('academic_search', self._search_academic_credentials)
        ]
        
        # Execute all search strategies
        for strategy_name, strategy_func in search_strategies:
            try:
                logger.info(f"Executing {strategy_name} for {clean_name}")
                strategy_result = strategy_func(clean_name, domain)
                if strategy_result:
                    self._safe_merge_results(result, strategy_result)
                    result['sources_checked'].append(strategy_name)
                    if strategy_result.get('found'):
                        result['found'] = True
            except Exception as e:
                logger.error(f"Error in {strategy_name}: {e}")
                continue
        
        # Additional targeted searches based on initial findings
        if result['found']:
            # Search for more details based on what we found
            self._targeted_deep_search(result, clean_name, domain)
        
        # Calculate comprehensive credibility score
        result['credibility_score'] = self._calculate_comprehensive_credibility_score(result)
        
        # Generate detailed credibility explanation
        result['credibility_explanation'] = self._generate_detailed_credibility_explanation(result)
        
        # Generate comprehensive bio if not found
        if not result['bio'] or 'Limited information' in result['bio']:
            result['bio'] = self._generate_comprehensive_bio(result, clean_name)
        
        # Cache the result
        self._cache_result(result, clean_name)
        
        logger.info(f"Completed analysis for {clean_name}. Found: {result['found']}, Score: {result['credibility_score']}")
        
        return result
    
    def _comprehensive_google_search(self, author_name, domain=None):
        """Comprehensive Google search with multiple query variations"""
        logger.info(f"Starting comprehensive Google search for {author_name}")
        
        result = {
            'found': False,
            'professional_info': {
                'outlets': [],
                'current_position': None,
                'expertise_areas': [],
                'previous_positions': []
            },
            'online_presence': {},
            'awards_recognition': {
                'awards': []
            },
            'publications': {
                'major_stories': []
            }
        }
        
        # Comprehensive search queries
        search_queries = [
            # Basic journalist searches
            f'"{author_name}" journalist bio',
            f'"{author_name}" reporter profile',
            f'"{author_name}" writer biography',
            f'"{author_name}" correspondent about',
            
            # Professional searches
            f'"{author_name}" LinkedIn journalist',
            f'"{author_name}" Muck Rack profile',
            f'"{author_name}" Contently portfolio',
            f'"{author_name}" journalism experience',
            
            # Awards and recognition
            f'"{author_name}" journalism award',
            f'"{author_name}" Pulitzer Prize',
            f'"{author_name}" press club award',
            f'"{author_name}" excellence journalism',
            
            # Education and credentials
            f'"{author_name}" journalism degree',
            f'"{author_name}" Columbia Journalism',
            f'"{author_name}" Northwestern Medill',
            f'"{author_name}" journalism education',
            
            # Published works
            f'"{author_name}" author book',
            f'"{author_name}" investigative reporting',
            f'"{author_name}" breaking news story',
            
            # Social media and online presence
            f'"{author_name}" Twitter journalist',
            f'"{author_name}" @{author_name.replace(" ", "")}',
            
            # Wikipedia and notable mentions
            f'"{author_name}" Wikipedia journalist',
            f'"{author_name}" notable journalist'
        ]
        
        # Add domain-specific searches
        if domain:
            search_queries.extend([
                f'"{author_name}" site:{domain}',
                f'"{author_name}" {domain} journalist profile',
                f'"{author_name}" {domain} author page'
            ])
        
        # Headers for requests
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Track what we've found to avoid duplicates
        found_info = {
            'bios': [],
            'positions': set(),
            'outlets': set(),
            'awards': set(),
            'education': set(),
            'stories': set()
        }
        
        # Perform searches
        for query in search_queries:
            try:
                # Add delay to avoid rate limiting
                time.sleep(0.5)
                
                encoded_query = quote(query)
                search_url = f"https://www.google.com/search?q={encoded_query}"
                
                response = self.session.get(search_url, headers=headers, timeout=10)
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Extract search results
                search_results = soup.select('div.g')
                
                for idx, result_div in enumerate(search_results[:10]):  # Check first 10 results
                    try:
                        # Get URL and snippet
                        link = result_div.select_one('a')
                        if not link or not link.get('href'):
                            continue
                        
                        url = link['href']
                        title_elem = result_div.select_one('h3')
                        title = title_elem.get_text() if title_elem else ''
                        snippet_elem = result_div.select_one('.VwiC3b')
                        snippet = snippet_elem.get_text() if snippet_elem else ''
                        
                        # Skip if author name not in snippet
                        if author_name.lower() not in snippet.lower() and author_name.lower() not in title.lower():
                            continue
                        
                        # Extract LinkedIn profile
                        if 'linkedin.com' in url and '/in/' in url:
                            result['online_presence']['linkedin'] = url
                            result['found'] = True
                            
                            # Extract from LinkedIn snippet
                            if ' at ' in snippet:
                                parts = snippet.split(' at ')
                                if len(parts) > 1:
                                    position = parts[0].split('-')[-1].strip()
                                    company = parts[1].split('-')[0].strip()
                                    if position and position not in found_info['positions']:
                                        found_info['positions'].add(position)
                                        if not result['professional_info']['current_position']:
                                            result['professional_info']['current_position'] = position
                                    if company:
                                        found_info['outlets'].add(company)
                        
                        # Extract Twitter/X profile
                        elif ('twitter.com' in url or 'x.com' in url) and author_name.lower() in url.lower():
                            match = re.search(r'(?:twitter\.com|x\.com)/([^/?\s]+)', url)
                            if match:
                                handle = match.group(1)
                                if handle not in ['search', 'i', 'home', 'explore']:
                                    result['online_presence']['twitter'] = handle
                                    result['found'] = True
                        
                        # Extract Muck Rack profile
                        elif 'muckrack.com' in url and author_name.lower().replace(' ', '-') in url.lower():
                            result['online_presence']['muckrack'] = url
                            result['found'] = True
                            result['verification_status']['journalist_verified'] = True
                        
                        # Extract Wikipedia
                        elif 'wikipedia.org' in url and author_name.lower() in title.lower():
                            result['online_presence']['wikipedia'] = url
                            result['found'] = True
                            result['verification_status']['verified'] = True
                        
                        # Extract awards
                        award_keywords = ['award', 'prize', 'winner', 'recipient', 'honored', 'excellence']
                        if any(keyword in snippet.lower() for keyword in award_keywords):
                            # Extract award details
                            award_patterns = [
                                r'won (?:the )?([^,\.]+(?:Award|Prize))',
                                r'received (?:the )?([^,\.]+(?:Award|Prize))',
                                r'recipient of (?:the )?([^,\.]+(?:Award|Prize))',
                                r'([^,\.]+(?:Award|Prize)) winner',
                                r'honored with (?:the )?([^,\.]+)'
                            ]
                            
                            for pattern in award_patterns:
                                matches = re.findall(pattern, snippet, re.IGNORECASE)
                                for match in matches:
                                    award = match.strip()
                                    if award and len(award) < 100 and award not in found_info['awards']:
                                        found_info['awards'].add(award)
                                        result['awards_recognition']['awards'].append({
                                            'name': award,
                                            'source': 'Google search',
                                            'snippet': snippet[:200]
                                        })
                                        result['verification_status']['award_winner'] = True
                                        result['found'] = True
                        
                        # Extract education
                        education_keywords = ['graduated', 'degree', 'studied', 'alumnus', 'alumni', 'BA', 'MA', 'PhD']
                        if any(keyword in snippet for keyword in education_keywords):
                            # Extract university names
                            university_patterns = [
                                r'graduated from ([^,\.]+(?:University|College|School))',
                                r'([^,\.]+(?:University|College)) graduate',
                                r'studied at ([^,\.]+(?:University|College))',
                                r'degree from ([^,\.]+(?:University|College))',
                                r'([^,\.]+(?:University|College)) alumnus'
                            ]
                            
                            for pattern in university_patterns:
                                matches = re.findall(pattern, snippet, re.IGNORECASE)
                                for match in matches:
                                    school = match.strip()
                                    if school and len(school) < 100 and school not in found_info['education']:
                                        found_info['education'].add(school)
                                        result['professional_info']['education'].append({
                                            'institution': school,
                                            'source': 'Google search'
                                        })
                                        result['found'] = True
                        
                        # Extract position and expertise
                        if 'is a' in snippet or 'is an' in snippet:
                            position_match = re.search(rf"{author_name}\s+is\s+(?:a|an|the)?\s*([^,\.]+?)(?:\s+at\s+([^,\.]+?))?[\.\,]", snippet, re.IGNORECASE)
                            if position_match:
                                position = position_match.group(1).strip()
                                outlet = position_match.group(2).strip() if position_match.group(2) else None
                                
                                if position and position not in found_info['positions']:
                                    found_info['positions'].add(position)
                                    if not result['professional_info']['current_position']:
                                        result['professional_info']['current_position'] = position
                                        result['found'] = True
                                
                                if outlet and outlet not in found_info['outlets']:
                                    found_info['outlets'].add(outlet)
                        
                        # Extract expertise areas
                        expertise_keywords = ['covers', 'reports on', 'writes about', 'specializes in', 'focuses on', 'beat includes', 'expertise in']
                        for keyword in expertise_keywords:
                            if keyword in snippet.lower():
                                pattern = rf"{keyword}\s+([^,\.]+)"
                                match = re.search(pattern, snippet.lower())
                                if match:
                                    expertise = match.group(1).strip()
                                    if expertise and len(expertise) < 50:
                                        result['professional_info']['expertise_areas'].append(expertise)
                                        result['found'] = True
                        
                        # Extract bio snippets
                        if len(snippet) > 100 and author_name in snippet:
                            found_info['bios'].append(snippet)
                        
                        # Try to fetch author pages directly
                        if any(term in url.lower() for term in ['/author/', '/journalist/', '/writer/', '/staff/', '/contributors/']):
                            try:
                                page_response = self.session.get(url, timeout=5)
                                if page_response.status_code == 200:
                                    page_soup = BeautifulSoup(page_response.text, 'html.parser')
                                    
                                    # Extract bio from page
                                    bio_selectors = [
                                        '[class*="bio"]', '[class*="author-desc"]', '[class*="author-info"]',
                                        '[class*="author-about"]', '.description', '.about'
                                    ]
                                    
                                    for selector in bio_selectors:
                                        bio_elem = page_soup.select_one(selector)
                                        if bio_elem:
                                            bio_text = bio_elem.get_text(strip=True)
                                            if len(bio_text) > 100 and author_name.lower() in bio_text.lower():
                                                found_info['bios'].append(bio_text)
                                                result['found'] = True
                                                break
                                    
                                    # Set author page URL
                                    if 'outlet_profile' not in result['online_presence']:
                                        result['online_presence']['outlet_profile'] = url
                            except:
                                pass
                    
                    except Exception as e:
                        logger.debug(f"Error processing search result: {e}")
                        continue
                
                # Check if we have enough information
                if len(found_info['awards']) >= 2 or len(found_info['education']) >= 1 or len(found_info['positions']) >= 3:
                    logger.info(f"Found substantial information for {author_name}, ending search early")
                    break
                    
            except Exception as e:
                logger.error(f"Error in Google search for query '{query}': {e}")
                continue
        
        # Compile results
        if found_info['outlets']:
            result['professional_info']['outlets'] = list(found_info['outlets'])[:10]
        
        if found_info['positions']:
            positions_list = list(found_info['positions'])
            if len(positions_list) > 1:
                result['professional_info']['previous_positions'] = positions_list[1:5]
        
        # Select best bio
        if found_info['bios']:
            # Prefer longer bios that mention credentials
            scored_bios = []
            for bio in found_info['bios']:
                score = len(bio)
                if 'award' in bio.lower():
                    score += 100
                if 'degree' in bio.lower() or 'graduated' in bio.lower():
                    score += 50
                if 'years' in bio.lower():
                    score += 50
                scored_bios.append((score, bio))
            
            scored_bios.sort(key=lambda x: x[0], reverse=True)
            result['bio'] = scored_bios[0][1]
        
        # Deduplicate expertise areas
        if result['professional_info']['expertise_areas']:
            result['professional_info']['expertise_areas'] = list(set(result['professional_info']['expertise_areas']))[:5]
        
        return result if result['found'] else None
    
    def _search_journalism_awards(self, author_name, domain=None):
        """Search for journalism awards and recognition"""
        logger.info(f"Searching for awards for {author_name}")
        
        result = {
            'awards_recognition': {
                'awards': [],
                'honors': []
            }
        }
        
        # Major journalism awards to check
        award_searches = [
            ('Pulitzer Prize', 'https://www.pulitzer.org/search/', 'pulitzer.org'),
            ('George Polk Award', None, 'George Polk'),
            ('Peabody Award', None, 'Peabody'),
            ('Edward R. Murrow Award', None, 'Murrow Award'),
            ('National Press Club', None, 'Press Club award'),
            ('SPJ Award', None, 'Society of Professional Journalists'),
            ('IRE Award', None, 'Investigative Reporters'),
            ('Online News Association', None, 'ONA award'),
            ('Emmy Award news', None, 'Emmy news'),
            ('duPont Award', None, 'duPont Columbia')
        ]
        
        for award_name, award_site, search_term in award_searches:
            try:
                query = f'"{author_name}" "{search_term or award_name}"'
                encoded_query = quote(query)
                search_url = f"https://www.google.com/search?q={encoded_query}"
                
                response = self.session.get(search_url, timeout=5)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Check if results mention the author and award
                    text = soup.get_text().lower()
                    if author_name.lower() in text and (search_term or award_name).lower() in text:
                        result['awards_recognition']['awards'].append({
                            'name': award_name,
                            'type': 'journalism',
                            'verified': award_site is not None
                        })
                        result['verification_status'] = {'award_winner': True}
                        result['found'] = True
                
                time.sleep(0.3)  # Rate limiting
                
            except Exception as e:
                logger.debug(f"Error searching for {award_name}: {e}")
                continue
        
        return result if result.get('found') else None
    
    def _search_education_background(self, author_name, domain=None):
        """Search for educational background"""
        logger.info(f"Searching education for {author_name}")
        
        result = {
            'professional_info': {
                'education': []
            }
        }
        
        # Top journalism schools to check
        journalism_schools = [
            'Columbia Journalism',
            'Northwestern Medill',
            'Missouri Journalism',
            'Syracuse Newhouse',
            'USC Annenberg',
            'NYU Journalism',
            'Berkeley Journalism',
            'Harvard Nieman',
            'Stanford Journalism',
            'University of Maryland Journalism'
        ]
        
        for school in journalism_schools:
            try:
                query = f'"{author_name}" "{school}"'
                encoded_query = quote(query)
                search_url = f"https://www.google.com/search?q={encoded_query}"
                
                response = self.session.get(search_url, timeout=5)
                if response.status_code == 200:
                    text = response.text.lower()
                    if author_name.lower() in text and school.lower() in text:
                        result['professional_info']['education'].append({
                            'institution': school,
                            'type': 'Journalism School',
                            'verified': True
                        })
                        result['found'] = True
                
                time.sleep(0.3)
                
            except Exception as e:
                logger.debug(f"Error searching {school}: {e}")
                continue
        
        return result if result.get('found') else None
    
    def _deep_linkedin_search(self, author_name, domain=None):
        """Deep LinkedIn search for professional information"""
        logger.info(f"Deep LinkedIn search for {author_name}")
        
        # Since we can't access LinkedIn directly without API, we search for LinkedIn profiles via Google
        query = f'site:linkedin.com/in/ "{author_name}" journalist OR reporter OR writer'
        if domain:
            query += f' "{domain}"'
        
        try:
            encoded_query = quote(query)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Look for LinkedIn URLs in results
                for link in soup.find_all('a', href=True):
                    href = link.get('href', '')
                    if 'linkedin.com/in/' in href:
                        # Extract the profile URL
                        match = re.search(r'(https?://[a-z]{2,3}\.linkedin\.com/in/[^&\s]+)', href)
                        if match:
                            linkedin_url = match.group(1)
                            
                            return {
                                'online_presence': {
                                    'linkedin': linkedin_url
                                },
                                'verification_status': {
                                    'verified': True
                                },
                                'found': True
                            }
        
        except Exception as e:
            logger.error(f"Error in LinkedIn search: {e}")
        
        return None
    
    def _analyze_twitter_presence(self, author_name, domain=None):
        """Analyze Twitter/X presence for verification"""
        logger.info(f"Analyzing Twitter presence for {author_name}")
        
        # Search for Twitter profile
        queries = [
            f'site:twitter.com "{author_name}" journalist',
            f'site:x.com "{author_name}" journalist',
            f'"@{author_name.replace(" ", "")}" journalist'
        ]
        
        for query in queries:
            try:
                encoded_query = quote(query)
                search_url = f"https://www.google.com/search?q={encoded_query}"
                
                response = self.session.get(search_url, timeout=5)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Extract Twitter handle from results
                    text = soup.get_text()
                    twitter_pattern = r'@(\w+)'
                    matches = re.findall(twitter_pattern, text)
                    
                    for handle in matches:
                        if handle.lower() != 'twitter' and len(handle) > 3:
                            # Verify this looks like the author's handle
                            if any(part.lower() in handle.lower() for part in author_name.split()):
                                return {
                                    'online_presence': {
                                        'twitter': handle
                                    },
                                    'found': True
                                }
                
                time.sleep(0.3)
                
            except Exception as e:
                logger.debug(f"Error in Twitter search: {e}")
                continue
        
        return None
    
    def _check_wikipedia(self, author_name, domain=None):
        """Check if author has Wikipedia page"""
        logger.info(f"Checking Wikipedia for {author_name}")
        
        try:
            # Search Wikipedia
            wiki_search = f'site:en.wikipedia.org "{author_name}" journalist OR reporter'
            encoded_query = quote(wiki_search)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Look for Wikipedia links
                for link in soup.find_all('a', href=True):
                    href = link.get('href', '')
                    if 'wikipedia.org/wiki/' in href and author_name.lower().replace(' ', '_') in href.lower():
                        # Extract Wikipedia URL
                        match = re.search(r'(https?://en\.wikipedia\.org/wiki/[^&\s]+)', href)
                        if match:
                            wiki_url = match.group(1)
                            
                            return {
                                'online_presence': {
                                    'wikipedia': wiki_url
                                },
                                'verification_status': {
                                    'verified': True,
                                    'journalist_verified': True
                                },
                                'found': True
                            }
        
        except Exception as e:
            logger.error(f"Error checking Wikipedia: {e}")
        
        return None
    
    def _check_all_journalism_databases(self, author_name):
        """Check multiple journalism databases"""
        logger.info(f"Checking journalism databases for {author_name}")
        
        databases_to_check = [
            ('Muck Rack', 'muckrack.com', self._check_muckrack),
            ('Contently', 'contently.com', self._check_contently),
            ('Presspass', 'presspass.com', self._check_presspass),
            ('Journo Portfolio', 'journoportfolio.com', self._check_journo_portfolio)
        ]
        
        result = {
            'online_presence': {},
            'verification_status': {}
        }
        found_any = False
        
        for db_name, db_domain, check_func in databases_to_check:
            try:
                db_result = check_func(author_name)
                if db_result:
                    result['online_presence'][db_name.lower().replace(' ', '_')] = db_result['url']
                    result['verification_status']['journalist_verified'] = True
                    found_any = True
            except Exception as e:
                logger.debug(f"Error checking {db_name}: {e}")
                continue
        
        if found_any:
            result['found'] = True
            return result
        
        return None
    
    def _check_muckrack(self, author_name):
        """Check Muck Rack for author profile"""
        try:
            # Try direct URL first
            author_slug = author_name.lower().replace(' ', '-')
            muckrack_url = f"https://muckrack.com/{author_slug}"
            
            response = self.session.get(muckrack_url, timeout=5)
            if response.status_code == 200:
                # Verify it's actually this author's page
                if author_name.lower() in response.text.lower():
                    return {'url': muckrack_url, 'found': True}
            
            # Try search
            search_query = f'site:muckrack.com "{author_name}"'
            encoded_query = quote(search_query)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=5)
            if response.status_code == 200 and author_name.lower() in response.text.lower():
                # Extract Muck Rack URL from search results
                match = re.search(r'(https?://muckrack\.com/[^"\s]+)', response.text)
                if match:
                    return {'url': match.group(1), 'found': True}
        
        except Exception as e:
            logger.debug(f"Error checking Muck Rack: {e}")
        
        return None
    
    def _check_contently(self, author_name):
        """Check Contently for author portfolio"""
        try:
            search_query = f'site:contently.com "{author_name}"'
            encoded_query = quote(search_query)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=5)
            if response.status_code == 200 and author_name.lower() in response.text.lower():
                match = re.search(r'(https?://[^.]+\.contently\.com)', response.text)
                if match:
                    return {'url': match.group(1), 'found': True}
        
        except Exception as e:
            logger.debug(f"Error checking Contently: {e}")
        
        return None
    
    def _check_presspass(self, author_name):
        """Check Presspass for author profile"""
        try:
            search_query = f'site:presspass.com "{author_name}"'
            encoded_query = quote(search_query)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=5)
            if response.status_code == 200 and author_name.lower() in response.text.lower():
                return {'url': 'https://presspass.com', 'found': True}
        
        except Exception as e:
            logger.debug(f"Error checking Presspass: {e}")
        
        return None
    
    def _check_journo_portfolio(self, author_name):
        """Check Journo Portfolio for author profile"""
        try:
            search_query = f'site:journoportfolio.com "{author_name}"'
            encoded_query = quote(search_query)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=5)
            if response.status_code == 200 and author_name.lower() in response.text.lower():
                return {'url': 'https://journoportfolio.com', 'found': True}
        
        except Exception as e:
            logger.debug(f"Error checking Journo Portfolio: {e}")
        
        return None
    
    def _search_published_books(self, author_name, domain=None):
        """Search for books published by the author"""
        logger.info(f"Searching for books by {author_name}")
        
        result = {
            'publications': {
                'books': []
            }
        }
        
        # Search queries for books
        book_queries = [
            f'"{author_name}" author book Amazon',
            f'"{author_name}" "has written" book',
            f'"{author_name}" "published" book journalism',
            f'site:goodreads.com "{author_name}" journalist'
        ]
        
        for query in book_queries:
            try:
                encoded_query = quote(query)
                search_url = f"https://www.google.com/search?q={encoded_query}"
                
                response = self.session.get(search_url, timeout=5)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    text = soup.get_text()
                    
                    # Look for book titles in quotes
                    book_pattern = r'"([^"]+)".*?(?:by|author|written)\s*' + re.escape(author_name)
                    matches = re.findall(book_pattern, text, re.IGNORECASE)
                    
                    for book_title in matches:
                        if len(book_title) > 5 and len(book_title) < 100:
                            result['publications']['books'].append({
                                'title': book_title,
                                'author': author_name,
                                'source': 'Google search'
                            })
                            result['verification_status'] = {'published_books': True}
                            result['found'] = True
                
                time.sleep(0.3)
                
            except Exception as e:
                logger.debug(f"Error searching for books: {e}")
                continue
        
        return result if result.get('found') else None
    
    def _search_academic_credentials(self, author_name, domain=None):
        """Search for academic credentials and teaching positions"""
        logger.info(f"Searching academic credentials for {author_name}")
        
        result = {
            'professional_info': {
                'education': [],
                'certifications': []
            }
        }
        
        # Academic search queries
        academic_queries = [
            f'"{author_name}" professor journalism',
            f'"{author_name}" "adjunct professor"',
            f'"{author_name}" "teaches journalism"',
            f'"{author_name}" PhD journalism',
            f'"{author_name}" "master degree" journalism'
        ]
        
        for query in academic_queries:
            try:
                encoded_query = quote(query)
                search_url = f"https://www.google.com/search?q={encoded_query}"
                
                response = self.session.get(search_url, timeout=5)
                if response.status_code == 200:
                    text = response.text
                    
                    # Extract academic positions
                    if 'professor' in text.lower() and author_name.lower() in text.lower():
                        # Try to extract university name
                        uni_pattern = r'(?:professor|teaches)\s+(?:at|in)\s+([^,\.]+(?:University|College))'
                        matches = re.findall(uni_pattern, text, re.IGNORECASE)
                        
                        for uni in matches:
                            result['professional_info']['education'].append({
                                'institution': uni.strip(),
                                'role': 'Professor/Instructor',
                                'type': 'Teaching Position'
                            })
                            result['found'] = True
                
                time.sleep(0.3)
                
            except Exception as e:
                logger.debug(f"Error in academic search: {e}")
                continue
        
        return result if result.get('found') else None
    
    def _targeted_deep_search(self, result, author_name, domain):
        """Perform targeted deep searches based on initial findings"""
        logger.info(f"Performing targeted deep search for {author_name}")
        
        # If we found awards, search for more details
        if result.get('awards_recognition', {}).get('awards'):
            for award in result['awards_recognition']['awards']:
                try:
                    query = f'"{author_name}" "{award["name"]}" year won details'
                    # Search for more details about the award
                    self._search_award_details(author_name, award['name'], result)
                except:
                    continue
        
        # If we found education, search for graduation year and degree
        if result.get('professional_info', {}).get('education'):
            for edu in result['professional_info']['education']:
                try:
                    query = f'"{author_name}" "{edu["institution"]}" graduated year degree'
                    # Search for more education details
                    self._search_education_details(author_name, edu['institution'], result)
                except:
                    continue
        
        # Calculate years of experience based on earliest mention
        self._estimate_years_experience(result, author_name)
    
    def _search_award_details(self, author_name, award_name, result):
        """Search for specific award details"""
        try:
            query = f'"{author_name}" "{award_name}" year category'
            encoded_query = quote(query)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=5)
            if response.status_code == 200:
                text = response.text
                
                # Try to extract year
                year_pattern = r'(19|20)\d{2}'
                years = re.findall(year_pattern, text)
                
                # Update award info with year if found
                for award in result['awards_recognition']['awards']:
                    if award['name'] == award_name and years:
                        award['year'] = years[0]
        
        except Exception as e:
            logger.debug(f"Error searching award details: {e}")
    
    def _search_education_details(self, author_name, institution, result):
        """Search for specific education details"""
        try:
            query = f'"{author_name}" "{institution}" graduated "class of"'
            encoded_query = quote(query)
            search_url = f"https://www.google.com/search?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=5)
            if response.status_code == 200:
                text = response.text
                
                # Try to extract graduation year
                year_pattern = r'(?:class of|graduated)\s*(19|20)\d{2}'
                matches = re.findall(year_pattern, text, re.IGNORECASE)
                
                # Update education info
                for edu in result['professional_info']['education']:
                    if edu['institution'] == institution and matches:
                        edu['graduation_year'] = matches[0]
        
        except Exception as e:
            logger.debug(f"Error searching education details: {e}")
    
    def _estimate_years_experience(self, result, author_name):
        """Estimate years of experience based on available data"""
        earliest_year = None
        
        # Check awards for years
        for award in result.get('awards_recognition', {}).get('awards', []):
            if 'year' in award:
                try:
                    year = int(award['year'])
                    if not earliest_year or year < earliest_year:
                        earliest_year = year
                except:
                    pass
        
        # Check education for graduation years
        for edu in result.get('professional_info', {}).get('education', []):
            if 'graduation_year' in edu:
                try:
                    year = int(edu['graduation_year'])
                    if not earliest_year or year < earliest_year:
                        earliest_year = year
                except:
                    pass
        
        # Calculate years of experience
        if earliest_year:
            current_year = datetime.now().year
            years_exp = current_year - earliest_year
            if years_exp > 0 and years_exp < 60:  # Reasonable range
                result['professional_info']['years_experience'] = years_exp
    
    def _calculate_comprehensive_credibility_score(self, author_data):
        """Calculate comprehensive credibility score with detailed weighting"""
        score = 0
        max_score = 100
        
        # Detailed scoring criteria
        criteria = {
            # Basic information (20 points)
            'has_bio': 5,
            'bio_quality': 5,  # Additional points for detailed bio
            'has_image': 2,
            'has_current_position': 8,
            
            # Professional credentials (25 points)
            'has_outlets': 5,
            'multiple_outlets': 5,  # Bonus for multiple outlets
            'years_experience': 10,
            'has_education': 5,
            
            # Verification and recognition (30 points)
            'outlet_verified': 10,
            'journalist_database': 10,
            'has_awards': 10,
            
            # Online presence (15 points)
            'has_linkedin': 5,
            'has_twitter': 3,
            'has_website': 3,
            'has_wikipedia': 4,
            
            # Additional credibility indicators (10 points)
            'published_books': 5,
            'academic_position': 5
        }
        
        # Apply scoring
        
        # Bio scoring
        if author_data.get('bio'):
            if 'Limited information' not in author_data['bio']:
                score += criteria['has_bio']
                if len(author_data['bio']) > 200:
                    score += criteria['bio_quality']
        
        # Image
        if author_data.get('image_url'):
            score += criteria['has_image']
        
        # Professional info
        prof_info = author_data.get('professional_info', {})
        
        if prof_info.get('current_position'):
            score += criteria['has_current_position']
        
        if prof_info.get('outlets'):
            score += criteria['has_outlets']
            if len(prof_info['outlets']) >= 3:
                score += criteria['multiple_outlets']
        
        if prof_info.get('years_experience'):
            years = prof_info['years_experience']
            if years >= 10:
                score += criteria['years_experience']
            elif years >= 5:
                score += criteria['years_experience'] // 2
        
        if prof_info.get('education'):
            score += criteria['has_education']
        
        # Verification status
        verification = author_data.get('verification_status', {})
        
        if verification.get('outlet_staff'):
            score += criteria['outlet_verified']
        
        if verification.get('journalist_verified'):
            score += criteria['journalist_database']
        
        if verification.get('award_winner'):
            score += criteria['has_awards']
        
        # Online presence
        online = author_data.get('online_presence', {})
        
        if online.get('linkedin'):
            score += criteria['has_linkedin']
        
        if online.get('twitter'):
            score += criteria['has_twitter']
        
        if online.get('personal_website'):
            score += criteria['has_website']
        
        if online.get('wikipedia'):
            score += criteria['has_wikipedia']
        
        # Additional indicators
        if verification.get('published_books'):
            score += criteria['published_books']
        
        # Check for academic position
        for edu in prof_info.get('education', []):
            if 'Professor' in edu.get('role', ''):
                score += criteria['academic_position']
                break
        
        return min(score, max_score)
    
    def _generate_detailed_credibility_explanation(self, author_data):
        """Generate detailed explanation of credibility assessment"""
        score = author_data.get('credibility_score', 0)
        
        # Analyze what we found
        has_awards = bool(author_data.get('awards_recognition', {}).get('awards'))
        has_education = bool(author_data.get('professional_info', {}).get('education'))
        has_verification = author_data.get('verification_status', {}).get('journalist_verified', False)
        years_exp = author_data.get('professional_info', {}).get('years_experience', 0)
        outlet_count = len(author_data.get('professional_info', {}).get('outlets', []))
        
        if score >= 85:
            level = 'Exceptional'
            explanation = f"Highly accomplished journalist with {years_exp}+ years experience" if years_exp else "Highly accomplished journalist"
            
            if has_awards:
                explanation += ", award-winning work"
            if has_education:
                explanation += ", strong educational credentials"
            if outlet_count >= 3:
                explanation += f", published in {outlet_count} major outlets"
            
            explanation += ". This author has exceptional credibility indicators across all measures."
            advice = "This is a highly credible journalist. Their work can be considered very reliable."
            
        elif score >= 70:
            level = 'High'
            explanation = "Well-established journalist with verified credentials"
            
            if has_verification:
                explanation += ", confirmed in journalism databases"
            if years_exp >= 5:
                explanation += f", {years_exp}+ years of experience"
            
            explanation += ". Strong professional track record."
            advice = "This author has strong credibility. Their reporting is likely reliable."
            
        elif score >= 50:
            level = 'Good'
            explanation = "Verified journalist with solid credentials"
            
            if outlet_count >= 2:
                explanation += f", work appears in {outlet_count} publications"
            
            explanation += ". Established professional presence."
            advice = "Author appears credible with reasonable verification. Standard journalistic reliability expected."
            
        elif score >= 30:
            level = 'Moderate'
            explanation = "Some professional information found"
            
            if author_data.get('found'):
                explanation += ", but limited verification available"
            
            explanation += ". Basic credibility indicators present."
            advice = "Limited verification available. Cross-reference important claims with other sources."
            
        else:
            level = 'Limited'
            
            if author_data.get('found'):
                explanation = "Minimal professional information found despite extensive searching."
            else:
                explanation = "No verifiable professional information found through comprehensive search."
            
            advice = "Unable to verify author credentials. Exercise caution and verify all claims independently."
        
        return {
            'level': level,
            'explanation': explanation,
            'advice': advice,
            'score_breakdown': self._get_score_breakdown(author_data)
        }
    
    def _get_score_breakdown(self, author_data):
        """Provide breakdown of what contributed to score"""
        breakdown = []
        
        if author_data.get('bio') and 'Limited information' not in author_data['bio']:
            breakdown.append(" Professional biography found")
        
        if author_data.get('professional_info', {}).get('current_position'):
            breakdown.append(" Current position verified")
        
        if author_data.get('professional_info', {}).get('outlets'):
            count = len(author_data['professional_info']['outlets'])
            breakdown.append(f" Published in {count} outlet(s)")
        
        if author_data.get('awards_recognition', {}).get('awards'):
            count = len(author_data['awards_recognition']['awards'])
            breakdown.append(f" {count} journalism award(s)")
        
        if author_data.get('professional_info', {}).get('education'):
            breakdown.append(" Educational credentials")
        
        if author_data.get('verification_status', {}).get('journalist_verified'):
            breakdown.append(" Verified in journalism databases")
        
        if author_data.get('online_presence', {}).get('linkedin'):
            breakdown.append(" LinkedIn profile")
        
        if author_data.get('online_presence', {}).get('wikipedia'):
            breakdown.append(" Wikipedia entry")
        
        return breakdown
    
    def _generate_comprehensive_bio(self, result, author_name):
        """Generate comprehensive bio from all collected information"""
        bio_parts = []
        
        # Start with name and current position
        if result['professional_info']['current_position']:
            bio_parts.append(f"{author_name} is {result['professional_info']['current_position']}")
            
            if result['professional_info']['outlets']:
                bio_parts.append(f"at {result['professional_info']['outlets'][0]}")
        else:
            bio_parts.append(f"{author_name} is a journalist")
        
        # Add experience
        if result['professional_info']['years_experience']:
            bio_parts.append(f"with {result['professional_info']['years_experience']} years of experience")
        
        # First sentence
        if bio_parts:
            bio = ' '.join(bio_parts) + '.'
        else:
            bio = f"{author_name} is a journalist."
        
        # Add expertise
        if result['professional_info']['expertise_areas']:
            areas = result['professional_info']['expertise_areas'][:3]
            bio += f" Specializes in {', '.join(areas)}."
        
        # Add awards
        if result['awards_recognition']['awards']:
            award_names = [a['name'] for a in result['awards_recognition']['awards'][:2]]
            bio += f" Award-winning journalist recognized with {', '.join(award_names)}."
        
        # Add education
        if result['professional_info']['education']:
            edu = result['professional_info']['education'][0]
            bio += f" Graduated from {edu['institution']}."
        
        # Add outlets if multiple
        if len(result['professional_info']['outlets']) > 1:
            other_outlets = result['professional_info']['outlets'][1:4]
            bio += f" Work has appeared in {', '.join(other_outlets)}."
        
        # Add books if any
        if result['publications']['books']:
            book_count = len(result['publications']['books'])
            bio += f" Author of {book_count} book{'s' if book_count > 1 else ''}."
        
        # Add verification status
        if result['verification_status']['journalist_verified']:
            bio += " Verified professional journalist."
        
        return bio
    
    def _cache_result(self, result, author_name):
        """Cache the comprehensive result"""
        try:
            from models import db, AuthorCache
            from datetime import timedelta
            
            # Remove existing cache entry if present
            AuthorCache.query.filter_by(author_name=author_name).delete()
            
            # Create new cache entry
            cache_entry = AuthorCache(
                author_name=author_name,
                lookup_data=result,
                expires_at=datetime.utcnow() + timedelta(days=30)
            )
            db.session.add(cache_entry)
            db.session.commit()
            
            logger.info(f"Cached author data for {author_name}")
            
        except Exception as e:
            logger.debug(f"Could not cache author data: {e}")
    
    # Keep all the existing helper methods from the original file
    def _safe_merge_results(self, target, source):
        """Safely merge source dict into target dict without overwriting None with empty values"""
        if not source:
            return
            
        for key, value in source.items():
            if key not in target:
                target[key] = value
            elif isinstance(value, dict) and isinstance(target[key], dict):
                # Recursively merge dictionaries
                self._safe_merge_results(target[key], value)
            elif isinstance(value, list) and isinstance(target[key], list):
                # Merge lists without duplicates
                for item in value:
                    if item not in target[key]:
                        target[key].append(item)
            elif value is not None and (not isinstance(value, (list, str)) or value):
                # Only update if source value is meaningful
                target[key] = value
    
    def _clean_author_name(self, author_name):
        """Clean and standardize author name"""
        if not author_name:
            return "Unknown"
            
        # Remove common suffixes
        name = re.sub(r'\s*(,|and|&)\s*.*', '', author_name)
        
        # Remove titles
        titles = ['Dr.', 'Prof.', 'Mr.', 'Mrs.', 'Ms.', 'Sir', 'Dame']
        for title in titles:
            name = name.replace(title, '').strip()
        
        # Remove extra whitespace
        name = ' '.join(name.split())
        
        return name if name else "Unknown"
    
    def _parse_authors(self, author_text):
        """Parse multiple authors from byline text"""
        if not author_text:
            return []
        
        # Split by common separators
        authors = re.split(r'\s*(?:,|and|&)\s*', author_text)
        
        # Clean each author
        cleaned_authors = []
        for author in authors:
            cleaned = self._clean_author_name(author)
            if cleaned and len(cleaned) > 2 and cleaned != "Unknown":
                cleaned_authors.append(cleaned)
        
        return cleaned_authors
    
    def _check_outlet_author_page(self, author_name, domain):
        """Check outlet's author page - keep existing implementation"""
        # Keep the existing implementation from your original file
        # This is already comprehensive
        clean_domain = domain.replace('www.', '')
        author_slug = author_name.lower().replace(' ', '-')
        author_underscore = author_name.lower().replace(' ', '_')
        author_plus = author_name.lower().replace(' ', '+')
        
        # Common author page URL patterns used by most news sites
        url_patterns = [
            f"https://{domain}/{author_slug}/",
            f"https://{domain}/author/{author_slug}/",
            f"https://{domain}/authors/{author_slug}/",
            f"https://{domain}/journalist/{author_slug}/",
            f"https://{domain}/journalists/{author_slug}/",
            f"https://{domain}/reporter/{author_slug}/",
            f"https://{domain}/staff/{author_slug}/",
            f"https://{domain}/contributors/{author_slug}/",
            f"https://{domain}/by/{author_slug}/",
            f"https://{domain}/profiles/{author_slug}/",
            f"https://{domain}/people/{author_slug}/",
            f"https://{domain}/team/{author_slug}/",
            f"https://{domain}/writer/{author_slug}/",
            f"https://www.{clean_domain}/{author_slug}/",
            f"https://www.{clean_domain}/author/{author_slug}/",
            # Try underscore versions
            f"https://{domain}/author/{author_underscore}/",
            f"https://{domain}/authors/{author_underscore}/",
            # Try plus sign versions
            f"https://{domain}/author/{author_plus}/",
        ]
        
        for url in url_patterns:
            try:
                response = self.session.get(url, timeout=5)
                if response.status_code == 200:
                    # Parse the page universally
                    result = self._parse_author_page_universal(response.text, url, domain, author_name)
                    if result and (result.get('bio') or result.get('found_author_page')):
                        return result
            except Exception as e:
                logger.debug(f"Failed to check {url}: {e}")
                continue
        
        return None
    
    def _parse_author_page_universal(self, html, url, domain, author_name):
        """Parse author page - keep existing implementation"""
        # Keep the existing comprehensive implementation from your original file
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Check if this is actually an author page (not a 404 or wrong page)
            page_text = soup.get_text().lower()
            author_name_lower = author_name.lower()
            
            # Verify this is likely an author page
            if (author_name_lower not in page_text and 
                not any(word in author_name_lower.split() for word in page_text.split()) and
                'page not found' not in page_text and 
                '404' not in page_text):
                return None
            
            result = {
                'online_presence': {'outlet_profile': url},
                'verification_status': {
                    'verified': True,
                    'journalist_verified': True,
                    'outlet_staff': True
                },
                'professional_info': {
                    'outlets': [domain.replace('www.', '').split('.')[0].upper()]
                },
                'found_author_page': True
            }
            
            # Rest of the existing implementation...
            # [Keep all the existing parsing logic from your original file]
            
            return result
        except Exception as e:
            logger.error(f"Error parsing author page: {e}")
            return None
