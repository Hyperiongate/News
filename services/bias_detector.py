"""
Bias Detector Service - OBJECTIVITY SCORING WITH OUTLET AWARENESS
Date: October 9, 2025
Version: 6.0.0 - OUTLET-AWARE BIAS DETECTION

CRITICAL CHANGES FROM v5.0.0:
- ADDED: Outlet baseline bias scoring (NY Post = right-leaning + sensationalist)
- ADDED: Controversial figure detection (RFK Jr., Alex Jones, etc.)
- ADDED: Pseudoscience indicator detection
- ADDED: Domain/source parameter acceptance
- ENHANCED: Sensationalism detection with outlet-specific baselines
- ENHANCED: Political bias detection with outlet context
- All existing objectivity scoring preserved (HIGHER = MORE OBJECTIVE)

THE FIX:
Now accepts 'domain' or 'source' in data to establish outlet baseline.
NY Post articles start with right-leaning + sensationalist baseline, then adjust based on content.
Reuters articles start with center + objective baseline.

This makes scoring intuitive: HIGHER IS ALWAYS BETTER
"""

import re
import logging
import time
import os
from typing import Dict, List, Any, Tuple, Optional
from collections import Counter
import statistics

from services.base_analyzer import BaseAnalyzer

logger = logging.getLogger(__name__)

# Import AI enhancement mixin if available
try:
    from services.ai_enhancement_mixin import AIEnhancementMixin
    AI_MIXIN_AVAILABLE = True
except ImportError:
    logger.info("AI Enhancement Mixin not available - running without AI enhancement")
    AIEnhancementMixin = object
    AI_MIXIN_AVAILABLE = False


class BiasDetector(BaseAnalyzer, AIEnhancementMixin):
    """
    OUTLET-AWARE OBJECTIVITY-FOCUSED BIAS DETECTION
    Now considers outlet baseline bias before analyzing article content
    Returns OBJECTIVITY scores where HIGHER = MORE OBJECTIVE
    """
    
    def __init__(self):
        """Initialize bias detector with comprehensive pattern database"""
        super().__init__('bias_detector')
        if AI_MIXIN_AVAILABLE:
            AIEnhancementMixin.__init__(self)
            self._ai_available = getattr(self, '_ai_available', False)
        else:
            self._ai_available = False
        
        # Initialize all bias patterns and indicators
        self._initialize_bias_patterns()
        self._initialize_outlet_baselines()
        self._initialize_controversial_figures()
        
        logger.info(f"BiasDetector v6.0.0 initialized (OUTLET-AWARE) with AI enhancement: {self._ai_available}")
    
    def _check_availability(self) -> bool:
        """Service is always available"""
        return True
    
    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Perform comprehensive bias analysis with OUTLET-AWARE OBJECTIVITY scoring
        HIGHER SCORES = MORE OBJECTIVE = BETTER
        """
        try:
            start_time = time.time()
            
            text = data.get('text', '')
            if not text:
                return self.get_error_result("No text provided for bias analysis")
            
            title = data.get('title', '')
            full_text = f"{title}\n\n{text}" if title else text
            
            # NEW v6.0: Extract outlet/domain information
            domain = data.get('domain', data.get('source', '')).lower().replace('www.', '')
            outlet_name = self._get_outlet_name(domain)
            
            logger.info(f"Analyzing objectivity in {len(full_text)} characters from {outlet_name or 'Unknown outlet'}")
            
            # NEW v6.0: Get outlet baseline bias
            outlet_baseline = self._get_outlet_baseline(domain, outlet_name)
            logger.info(f"Outlet baseline: {outlet_baseline['bias_direction']}, "
                       f"bias_score: {outlet_baseline['bias_amount']}, "
                       f"sensationalism: {outlet_baseline['sensationalism_baseline']}")
            
            # Core bias analysis with timeout protection
            try:
                political_bias = self._analyze_political_bias(full_text, outlet_baseline)
                sensationalism = self._analyze_sensationalism(full_text, title, outlet_baseline)
                corporate_bias = self._analyze_corporate_bias(full_text)
                loaded_language = self._detect_loaded_language(full_text)
                framing_analysis = self._analyze_framing(full_text)
                
                # NEW v6.0: Controversial figure detection
                controversial_figures = self._detect_controversial_figures(full_text)
                
                # NEW v6.0: Pseudoscience detection
                pseudoscience = self._detect_pseudoscience(full_text)
                
            except Exception as analysis_error:
                logger.warning(f"Bias analysis components failed: {analysis_error}")
                # Provide minimal analysis if detailed analysis fails
                political_bias = {'label': outlet_baseline['bias_direction'].title(), 
                                'score': outlet_baseline['bias_amount']}
                sensationalism = {'level': 'Low', 'score': outlet_baseline['sensationalism_baseline']}
                corporate_bias = {'bias': 'Neutral', 'score': 0}
                loaded_language = {'phrases': [], 'count': 0}
                framing_analysis = {'issues': []}
                controversial_figures = {'found': [], 'count': 0}
                pseudoscience = {'indicators': [], 'score': 0}
            
            # Calculate overall bias metrics
            bias_dimensions = {
                'political': political_bias,
                'sensationalism': sensationalism,
                'corporate': corporate_bias,
                'loaded_language': loaded_language,
                'framing': framing_analysis,
                'controversial_figures': controversial_figures,
                'pseudoscience': pseudoscience,
                'outlet_baseline': outlet_baseline
            }
            
            # Calculate BIAS amount (for internal use)
            overall_bias_amount = self._calculate_overall_bias_score(bias_dimensions)
            
            # CRITICAL: Convert to OBJECTIVITY score (invert)
            objectivity_score = 100 - overall_bias_amount
            
            objectivity_level = self._get_objectivity_level(objectivity_score)
            bias_direction = self._get_bias_direction(political_bias)
            
            # Generate findings (with objectivity focus and outlet context)
            findings = self._generate_objectivity_findings(bias_dimensions, objectivity_score, outlet_name)
            
            # Generate summary (with objectivity focus and outlet context)
            summary = self._generate_objectivity_summary(bias_dimensions, objectivity_score, 
                                                        objectivity_level, outlet_name)
            
            # Build response with OBJECTIVITY scoring
            result = {
                'service': self.service_name,
                'success': True,
                'available': True,
                'timestamp': time.time(),
                'analysis_complete': True,
                'data': {
                    # PRIMARY SCORES - Objectivity (higher is better)
                    'score': objectivity_score,
                    'objectivity_score': objectivity_score,
                    'level': objectivity_level,
                    'objectivity_level': objectivity_level,
                    
                    # LEGACY FIELDS - Bias amount (for backward compatibility)
                    'bias_score': overall_bias_amount,
                    'bias_level': self._get_bias_level(overall_bias_amount),
                    
                    # Analysis results
                    'findings': findings,
                    'summary': summary,
                    'dimensions': bias_dimensions,
                    'loaded_phrases': loaded_language.get('phrases', [])[:10],
                    'dominant_issue': self._get_dominant_bias(bias_dimensions),
                    'political_label': political_bias.get('label', 'Center'),
                    'sensationalism_level': sensationalism.get('level', 'Low'),
                    'bias_direction': bias_direction,
                    'political_leaning': political_bias.get('label', 'Center'),
                    'patterns': self._get_bias_patterns_summary(bias_dimensions),
                    
                    # NEW v6.0: Outlet context
                    'outlet_name': outlet_name,
                    'outlet_baseline': outlet_baseline,
                    
                    # NEW v6.0: Enhanced detection
                    'controversial_figures': controversial_figures.get('found', []),
                    'pseudoscience_detected': pseudoscience.get('score', 0) > 20,
                    
                    'details': {
                        'political_score': political_bias.get('score', 0),
                        'sensationalism_score': sensationalism.get('score', 0),
                        'corporate_score': corporate_bias.get('score', 0),
                        'loaded_language_count': len(loaded_language.get('phrases', [])),
                        'framing_issues': len(framing_analysis.get('issues', [])),
                        'controversial_figures_count': controversial_figures.get('count', 0),
                        'pseudoscience_score': pseudoscience.get('score', 0)
                    }
                },
                'metadata': {
                    'analysis_time': time.time() - start_time,
                    'text_length': len(full_text),
                    'analyzed_with_title': bool(title),
                    'ai_enhancement_attempted': self._ai_available,
                    'scoring_type': 'objectivity',
                    'outlet_aware': bool(outlet_name),
                    'version': '6.0.0'
                }
            }
            
            # BULLETPROOF AI ENHANCEMENT (if available)
            if full_text and self._ai_available and AI_MIXIN_AVAILABLE:
                logger.info("Enhancing bias detection with AI insights")
                try:
                    enhanced_result = self._safely_enhance_service_result(
                        result,
                        '_ai_detect_bias_patterns',
                        text=full_text[:1500],
                        initial_findings=result['data']
                    )
                    if enhanced_result:
                        result = enhanced_result
                        result['metadata']['ai_enhancement_applied'] = True
                except Exception as ai_error:
                    logger.warning(f"AI enhancement failed safely: {ai_error}")
                    result['metadata']['ai_enhancement_failed'] = str(ai_error)
            
            logger.info(f"Objectivity analysis complete: {objectivity_score}/100 ({objectivity_level}) for {outlet_name}")
            return result
            
        except Exception as e:
            logger.error(f"Bias analysis failed: {e}", exc_info=True)
            return self.get_error_result(str(e))
    
    def _initialize_bias_patterns(self):
        """Initialize comprehensive bias detection patterns"""
        
        # Political bias indicators
        self.political_patterns = {
            'left_indicators': [
                'progressive', 'liberal', 'social justice', 'inequality', 'climate crisis',
                'systemic racism', 'corporate greed', 'wealth gap', 'social programs',
                'reproductive rights', 'gun control', 'universal healthcare', 'living wage',
                'social safety net', 'workers rights', 'income inequality', 'marginalized communities'
            ],
            'right_indicators': [
                'conservative', 'traditional values', 'free market', 'personal responsibility',
                'law and order', 'strong defense', 'fiscal responsibility', 'family values',
                'second amendment', 'limited government', 'individual liberty', 'tax cuts',
                'border security', 'religious freedom', 'constitutional rights', 'fiscal conservative'
            ],
            'extremist_indicators': [
                'radical', 'extremist', 'deep state', 'mainstream media', 'establishment',
                'wake up', 'sheeple', 'crisis actor', 'false flag', 'globalist', 'marxist',
                'communist threat', 'radical left', 'far-right', 'antifa', 'woke mob'
            ]
        }
        
        # Sensationalism patterns
        self.sensationalism_patterns = [
            'shocking', 'explosive', 'bombshell', 'devastating', 'unprecedented',
            'crisis', 'disaster', 'scandal', 'outrageous', 'incredible', 'stunning',
            'breaking', 'urgent', 'must-see', 'viral', 'epic', 'massive', 'huge',
            'terrifying', 'alarming', 'horrifying', 'unbelievable', 'insane',
            'slams', 'blasts', 'destroys', 'annihilates', 'crushes', 'eviscerates'
        ]
        
        # Corporate bias indicators
        self.corporate_patterns = {
            'pro_business': [
                'innovation', 'job creation', 'economic growth', 'efficiency',
                'competitive advantage', 'market leader', 'shareholder value',
                'entrepreneurship', 'free enterprise', 'business friendly'
            ],
            'anti_business': [
                'corporate greed', 'exploitation', 'price gouging', 'monopoly',
                'tax avoidance', 'worker exploitation', 'environmental destruction',
                'predatory practices', 'wage theft', 'corporate welfare'
            ]
        }
        
        # Loaded language patterns
        self.loaded_patterns = [
            'alleged', 'claimed', 'so-called', 'notorious', 'infamous',
            'controversial', 'divisive', 'polarizing', 'radical', 'extreme',
            'supposedly', 'purportedly', 'apparently'
        ]
    
    def _initialize_outlet_baselines(self):
        """NEW v6.0: Initialize outlet bias baselines"""
        
        self.outlet_baselines = {
            # High objectivity outlets
            'reuters.com': {'bias_direction': 'center', 'bias_amount': 5, 'sensationalism': 0},
            'apnews.com': {'bias_direction': 'center', 'bias_amount': 5, 'sensationalism': 0},
            'bbc.com': {'bias_direction': 'center', 'bias_amount': 10, 'sensationalism': 5},
            'bbc.co.uk': {'bias_direction': 'center', 'bias_amount': 10, 'sensationalism': 5},
            
            # Center-left outlets
            'npr.org': {'bias_direction': 'center-left', 'bias_amount': 15, 'sensationalism': 0},
            'nytimes.com': {'bias_direction': 'left', 'bias_amount': 20, 'sensationalism': 10},
            'washingtonpost.com': {'bias_direction': 'left', 'bias_amount': 20, 'sensationalism': 10},
            'theguardian.com': {'bias_direction': 'left', 'bias_amount': 22, 'sensationalism': 12},
            'cnn.com': {'bias_direction': 'left', 'bias_amount': 25, 'sensationalism': 20},
            'msnbc.com': {'bias_direction': 'left', 'bias_amount': 35, 'sensationalism': 25},
            'vox.com': {'bias_direction': 'left', 'bias_amount': 30, 'sensationalism': 15},
            'huffpost.com': {'bias_direction': 'left', 'bias_amount': 32, 'sensationalism': 28},
            'salon.com': {'bias_direction': 'left', 'bias_amount': 35, 'sensationalism': 30},
            'motherjones.com': {'bias_direction': 'left', 'bias_amount': 33, 'sensationalism': 20},
            
            # Center-right outlets
            'wsj.com': {'bias_direction': 'center-right', 'bias_amount': 20, 'sensationalism': 8},
            'economist.com': {'bias_direction': 'center-right', 'bias_amount': 18, 'sensationalism': 5},
            
            # Right-leaning outlets
            'foxnews.com': {'bias_direction': 'right', 'bias_amount': 35, 'sensationalism': 30},
            'nypost.com': {'bias_direction': 'right', 'bias_amount': 30, 'sensationalism': 40},  # KEY FIX
            'dailywire.com': {'bias_direction': 'right', 'bias_amount': 38, 'sensationalism': 25},
            'theblaze.com': {'bias_direction': 'right', 'bias_amount': 36, 'sensationalism': 28},
            'newsmax.com': {'bias_direction': 'right', 'bias_amount': 42, 'sensationalism': 35},
            
            # Far-right outlets
            'breitbart.com': {'bias_direction': 'far-right', 'bias_amount': 50, 'sensationalism': 45},
            'oann.com': {'bias_direction': 'far-right', 'bias_amount': 55, 'sensationalism': 40},
            
            # Tabloids
            'dailymail.co.uk': {'bias_direction': 'right', 'bias_amount': 35, 'sensationalism': 50},
            
            # Mainstream broadcast
            'abcnews.go.com': {'bias_direction': 'center-left', 'bias_amount': 15, 'sensationalism': 12},
            'nbcnews.com': {'bias_direction': 'center-left', 'bias_amount': 18, 'sensationalism': 15},
            'cbsnews.com': {'bias_direction': 'center-left', 'bias_amount': 16, 'sensationalism': 13},
            
            # Digital news
            'politico.com': {'bias_direction': 'center-left', 'bias_amount': 18, 'sensationalism': 10},
            'axios.com': {'bias_direction': 'center', 'bias_amount': 12, 'sensationalism': 8},
            'thehill.com': {'bias_direction': 'center-left', 'bias_amount': 15, 'sensationalism': 12}
        }
    
    def _initialize_controversial_figures(self):
        """NEW v6.0: Initialize controversial figure detection"""
        
        self.controversial_figures = {
            # Pseudoscience promoters
            'rfk jr': {'category': 'vaccine skeptic', 'weight': 15},
            'robert f. kennedy jr': {'category': 'vaccine skeptic', 'weight': 15},
            'robert kennedy': {'category': 'vaccine skeptic', 'weight': 10},
            'alex jones': {'category': 'conspiracy theorist', 'weight': 20},
            'joe mercola': {'category': 'pseudoscience', 'weight': 18},
            'dr. mercola': {'category': 'pseudoscience', 'weight': 18},
            'dr. oz': {'category': 'pseudoscience', 'weight': 12},
            'mehmet oz': {'category': 'pseudoscience', 'weight': 12},
            'gwyneth paltrow': {'category': 'pseudoscience', 'weight': 10},
            
            # Political extremists
            'steve bannon': {'category': 'far-right', 'weight': 15},
            'tucker carlson': {'category': 'controversial commentator', 'weight': 12},
            'marjorie taylor greene': {'category': 'conspiracy theorist', 'weight': 18},
            'lauren boebert': {'category': 'conspiracy theorist', 'weight': 15},
            
            # Note: This is NOT a political statement - these are figures frequently
            # associated with misinformation by fact-checkers
        }
        
        # Pseudoscience indicators
        self.pseudoscience_indicators = [
            'big pharma conspiracy', 'mainstream medicine', 'natural immunity', 
            'vaccine injury', 'toxins', 'detox', 'chemtrails', 'fluoride conspiracy',
            'suppressed cure', 'they don\'t want you to know', 'hidden truth',
            'miracle cure', 'ancient remedy', 'pharmaceutical industry cover-up',
            'natural healing', 'alternative facts', 'do your own research'
        ]
    
    def _get_outlet_name(self, domain: str) -> Optional[str]:
        """Convert domain to readable outlet name"""
        if not domain:
            return None
            
        name_mapping = {
            'reuters.com': 'Reuters',
            'apnews.com': 'Associated Press',
            'bbc.com': 'BBC',
            'bbc.co.uk': 'BBC',
            'nytimes.com': 'The New York Times',
            'washingtonpost.com': 'The Washington Post',
            'wsj.com': 'The Wall Street Journal',
            'cnn.com': 'CNN',
            'foxnews.com': 'Fox News',
            'msnbc.com': 'MSNBC',
            'npr.org': 'NPR',
            'nypost.com': 'New York Post',
            'dailymail.co.uk': 'Daily Mail',
            'breitbart.com': 'Breitbart',
            'politico.com': 'Politico',
            'thehill.com': 'The Hill',
            'axios.com': 'Axios',
            'vox.com': 'Vox',
            'theguardian.com': 'The Guardian'
        }
        
        return name_mapping.get(domain)
    
    def _get_outlet_baseline(self, domain: str, outlet_name: Optional[str]) -> Dict[str, Any]:
        """NEW v6.0: Get outlet baseline bias"""
        
        if domain in self.outlet_baselines:
            baseline = self.outlet_baselines[domain].copy()
            baseline['source'] = 'database'
            return {
                'bias_direction': baseline['bias_direction'],
                'bias_amount': baseline['bias_amount'],
                'sensationalism_baseline': baseline['sensationalism'],
                'known_outlet': True,
                'outlet_name': outlet_name or domain
            }
        
        # Default baseline for unknown outlets
        return {
            'bias_direction': 'center',
            'bias_amount': 15,  # Assume moderate baseline
            'sensationalism_baseline': 10,
            'known_outlet': False,
            'outlet_name': outlet_name or 'Unknown'
        }
    
    def _detect_controversial_figures(self, text: str) -> Dict[str, Any]:
        """NEW v6.0: Detect mentions of controversial figures"""
        
        text_lower = text.lower()
        found_figures = []
        total_weight = 0
        
        for figure, info in self.controversial_figures.items():
            if figure in text_lower:
                found_figures.append({
                    'name': figure.title(),
                    'category': info['category'],
                    'weight': info['weight']
                })
                total_weight += info['weight']
        
        return {
            'found': found_figures,
            'count': len(found_figures),
            'bias_impact': min(25, total_weight)  # Cap at 25 points
        }
    
    def _detect_pseudoscience(self, text: str) -> Dict[str, Any]:
        """NEW v6.0: Detect pseudoscience indicators"""
        
        text_lower = text.lower()
        found_indicators = []
        
        for indicator in self.pseudoscience_indicators:
            if indicator in text_lower:
                found_indicators.append(indicator)
        
        # Calculate pseudoscience score
        score = min(30, len(found_indicators) * 8)
        
        return {
            'indicators': found_indicators,
            'count': len(found_indicators),
            'score': score
        }
    
    def _analyze_political_bias(self, text: str, outlet_baseline: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze political bias with outlet baseline context"""
        try:
            text_lower = text.lower()
            
            # Start with outlet baseline
            base_bias = outlet_baseline['bias_amount']
            base_direction = outlet_baseline['bias_direction']
            
            # Count indicators in text
            left_score = sum(1 for indicator in self.political_patterns['left_indicators'] 
                            if indicator in text_lower)
            right_score = sum(1 for indicator in self.political_patterns['right_indicators'] 
                             if indicator in text_lower)
            extremist_score = sum(1 for indicator in self.political_patterns['extremist_indicators'] 
                                 if indicator in text_lower)
            
            # Calculate article-specific lean
            total_political = left_score + right_score
            article_lean = 0
            article_direction = base_direction
            
            if total_political > 0:
                if left_score > right_score * 1.5:
                    article_lean = min(25, (left_score / total_political) * 40)
                    article_direction = 'left' if article_lean > 15 else 'center-left'
                elif right_score > left_score * 1.5:
                    article_lean = min(25, (right_score / total_political) * 40)
                    article_direction = 'right' if article_lean > 15 else 'center-right'
            
            # Combine outlet baseline with article lean
            if 'left' in base_direction and 'right' in article_direction:
                # Article goes against outlet bias - reduce bias
                final_score = max(5, base_bias - article_lean)
                final_direction = 'center'
            elif 'right' in base_direction and 'left' in article_direction:
                # Article goes against outlet bias - reduce bias
                final_score = max(5, base_bias - article_lean)
                final_direction = 'center'
            else:
                # Article reinforces outlet bias - increase bias
                final_score = min(60, base_bias + article_lean)
                final_direction = article_direction if article_direction != 'center' else base_direction
            
            # Add extremist adjustment
            extremist_adjustment = min(20, extremist_score * 10)
            final_score = min(60, final_score + extremist_adjustment)
            
            # Determine label
            if final_score >= 45:
                political_lean = 'Far-' + final_direction.title() if final_direction != 'center' else 'Extreme'
            elif final_score >= 30:
                political_lean = final_direction.title()
            elif final_score >= 18:
                political_lean = 'Center-' + final_direction.split('-')[-1].title() if '-' in final_direction else final_direction.title()
            else:
                political_lean = 'Center'
            
            return {
                'label': political_lean,
                'score': int(final_score),
                'left_indicators': left_score,
                'right_indicators': right_score,
                'extremist_indicators': extremist_score,
                'outlet_baseline': base_bias,
                'article_adjustment': int(article_lean)
            }
        except Exception as e:
            logger.warning(f"Political bias analysis failed: {e}")
            return {'label': 'Center', 'score': 15}
    
    def _analyze_sensationalism(self, text: str, title: str, outlet_baseline: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze sensationalism with outlet baseline"""
        try:
            text_lower = text.lower()
            title_lower = title.lower() if title else ''
            
            # Start with outlet baseline
            base_sensationalism = outlet_baseline['sensationalism_baseline']
            
            # Count sensational words
            sensational_count = sum(1 for pattern in self.sensationalism_patterns 
                                   if pattern in text_lower)
            
            # Extra weight for headline sensationalism (2x impact)
            title_sensational = sum(1 for pattern in self.sensationalism_patterns 
                                   if pattern in title_lower)
            
            # Calculate sensationalism score based on density
            word_count = len(text.split())
            if word_count == 0:
                return {'score': base_sensationalism, 'level': 'Minimal'}
            
            sensational_density = (sensational_count / word_count) * 1000
            article_sensationalism = int(sensational_density * 10)
            
            # Title sensationalism gets extra weight
            if title_sensational > 0:
                article_sensationalism += title_sensational * 15
            
            # Combine with outlet baseline
            final_score = min(100, base_sensationalism + article_sensationalism)
            
            if final_score >= 70:
                level = 'Very High'
            elif final_score >= 50:
                level = 'High'
            elif final_score >= 30:
                level = 'Moderate'
            elif final_score >= 15:
                level = 'Low'
            else:
                level = 'Minimal'
            
            return {
                'score': int(final_score),
                'level': level,
                'sensational_phrases': sensational_count,
                'title_sensationalism': title_sensational,
                'density': sensational_density,
                'outlet_baseline': base_sensationalism,
                'article_contribution': article_sensationalism
            }
        except Exception as e:
            logger.warning(f"Sensationalism analysis failed: {e}")
            return {'score': 15, 'level': 'Low'}
    
    def _analyze_corporate_bias(self, text: str) -> Dict[str, Any]:
        """Analyze corporate/business bias"""
        try:
            text_lower = text.lower()
            
            pro_business = sum(1 for indicator in self.corporate_patterns['pro_business'] 
                              if indicator in text_lower)
            anti_business = sum(1 for indicator in self.corporate_patterns['anti_business'] 
                               if indicator in text_lower)
            
            total_corporate = pro_business + anti_business
            if total_corporate == 0:
                corporate_bias = 'Neutral'
                score = 0
            elif pro_business > anti_business:
                score = min(100, (pro_business / total_corporate) * 100)
                corporate_bias = 'Pro-Business'
            else:
                score = min(100, (anti_business / total_corporate) * 100)
                corporate_bias = 'Anti-Business'
            
            return {
                'bias': corporate_bias,
                'score': int(score),
                'pro_business_indicators': pro_business,
                'anti_business_indicators': anti_business
            }
        except Exception as e:
            logger.warning(f"Corporate bias analysis failed: {e}")
            return {'bias': 'Neutral', 'score': 0}
    
    def _detect_loaded_language(self, text: str) -> Dict[str, Any]:
        """Detect loaded/biased language"""
        try:
            sentences = re.split(r'[.!?]+', text)
            
            loaded_phrases = []
            for sentence in sentences[:100]:
                sentence_lower = sentence.lower()
                for pattern in self.loaded_patterns:
                    if pattern in sentence_lower:
                        context = sentence.strip()
                        if len(context) > 10:
                            loaded_phrases.append({
                                'phrase': pattern,
                                'context': context[:200],
                                'sentence': context
                            })
            
            # Remove duplicates
            unique_phrases = []
            seen_contexts = set()
            for phrase in loaded_phrases:
                if phrase['context'] not in seen_contexts:
                    unique_phrases.append(phrase)
                    seen_contexts.add(phrase['context'])
            
            return {
                'phrases': unique_phrases[:15],
                'count': len(unique_phrases),
                'density': len(unique_phrases) / max(len(sentences), 1)
            }
        except Exception as e:
            logger.warning(f"Loaded language detection failed: {e}")
            return {'phrases': [], 'count': 0, 'density': 0}
    
    def _analyze_framing(self, text: str) -> Dict[str, Any]:
        """Analyze how the article frames issues"""
        try:
            framing_issues = []
            text_lower = text.lower()
            
            # One-sided framing
            if 'however' not in text_lower and 'but' not in text_lower and len(text) > 500:
                framing_issues.append("Limited counterarguments presented")
            
            # Emotional framing
            emotional_words = ['outraged', 'devastated', 'thrilled', 'shocked', 'horrified']
            emotional_count = sum(1 for word in emotional_words if word in text_lower)
            if emotional_count > 3:
                framing_issues.append("Heavy emotional language usage")
            
            # Source diversity
            quote_count = text.count('"')
            source_indicators = ['according to', 'said', 'stated', 'reported']
            source_count = sum(1 for indicator in source_indicators if indicator in text_lower)
            
            if quote_count > 0 and source_count < 2:
                framing_issues.append("Limited source diversity")
            
            return {
                'issues': framing_issues,
                'emotional_language_count': emotional_count,
                'source_diversity_score': min(100, source_count * 25)
            }
        except Exception as e:
            logger.warning(f"Framing analysis failed: {e}")
            return {'issues': [], 'emotional_language_count': 0, 'source_diversity_score': 0}
    
    def _calculate_overall_bias_score(self, dimensions: Dict[str, Any]) -> int:
        """Calculate overall BIAS amount (internal use - will be inverted for objectivity)"""
        try:
            political_score = dimensions['political'].get('score', 0)
            sensationalism_score = dimensions['sensationalism'].get('score', 0)
            corporate_score = dimensions['corporate'].get('score', 0)
            loaded_count = dimensions['loaded_language'].get('count', 0)
            framing_issues = len(dimensions['framing'].get('issues', []))
            
            # NEW v6.0: Add controversial figures and pseudoscience
            controversial_impact = dimensions.get('controversial_figures', {}).get('bias_impact', 0)
            pseudoscience_score = dimensions.get('pseudoscience', {}).get('score', 0)
            
            # Weight different types of bias
            weighted_score = (
                political_score * 0.25 +
                sensationalism_score * 0.25 +
                corporate_score * 0.15 +
                min(100, loaded_count * 5) * 0.10 +
                min(100, framing_issues * 15) * 0.08 +
                controversial_impact * 0.10 +
                pseudoscience_score * 0.07
            )
            
            return min(100, int(weighted_score))
        except Exception as e:
            logger.warning(f"Bias score calculation failed: {e}")
            return 25
    
    def _get_objectivity_level(self, score: int) -> str:
        """Convert objectivity score to level (HIGHER IS BETTER)"""
        if score >= 85:
            return 'Highly Objective'
        elif score >= 70:
            return 'Very Objective'
        elif score >= 50:
            return 'Moderately Objective'
        elif score >= 30:
            return 'Some Bias Present'
        else:
            return 'Significant Bias'
    
    def _get_bias_level(self, score: int) -> str:
        """Convert bias amount to level (for legacy compatibility)"""
        if score >= 70:
            return 'High Bias'
        elif score >= 50:
            return 'Moderate Bias'
        elif score >= 30:
            return 'Low Bias'
        elif score >= 15:
            return 'Minimal Bias'
        else:
            return 'Very Low Bias'
    
    def _get_dominant_bias(self, dimensions: Dict[str, Any]) -> str:
        """Identify the dominant type of bias"""
        try:
            scores = {
                'Political': dimensions['political'].get('score', 0),
                'Sensationalism': dimensions['sensationalism'].get('score', 0),
                'Corporate': dimensions['corporate'].get('score', 0),
                'Loaded Language': min(100, dimensions['loaded_language'].get('count', 0) * 10)
            }
            
            # NEW v6.0: Add controversial figures if significant
            if dimensions.get('controversial_figures', {}).get('bias_impact', 0) > 15:
                scores['Controversial Figures'] = dimensions['controversial_figures']['bias_impact']
            
            if max(scores.values()) < 20:
                return 'None'
            
            return max(scores, key=scores.get)
        except Exception as e:
            logger.warning(f"Dominant bias calculation failed: {e}")
            return 'Unknown'
    
    def _get_bias_direction(self, political_bias: Dict[str, Any]) -> str:
        """Get bias direction from political analysis"""
        try:
            label = political_bias.get('label', 'Center')
            if 'Left' in label:
                return 'left'
            elif 'Right' in label:
                return 'right'
            else:
                return 'center'
        except Exception:
            return 'center'
    
    def _get_bias_patterns_summary(self, dimensions: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Get summary of detected bias patterns"""
        try:
            patterns = []
            
            political = dimensions['political']
            if political.get('score', 0) > 30:
                patterns.append({
                    'type': 'political',
                    'description': f"Political lean: {political.get('label', 'Unknown')}",
                    'strength': political.get('score', 0)
                })
            
            sensationalism = dimensions['sensationalism']
            if sensationalism.get('score', 0) > 40:
                patterns.append({
                    'type': 'sensationalism',
                    'description': f"Sensational language: {sensationalism.get('level', 'Unknown')}",
                    'strength': sensationalism.get('score', 0)
                })
            
            loaded = dimensions['loaded_language']
            if loaded.get('count', 0) > 3:
                patterns.append({
                    'type': 'loaded_language',
                    'description': f"Loaded language: {loaded.get('count', 0)} instances",
                    'strength': min(100, loaded.get('count', 0) * 10)
                })
            
            # NEW v6.0: Controversial figures
            controversial = dimensions.get('controversial_figures', {})
            if controversial.get('count', 0) > 0:
                patterns.append({
                    'type': 'controversial_figures',
                    'description': f"Controversial figures: {controversial.get('count', 0)} mentioned",
                    'strength': controversial.get('bias_impact', 0)
                })
            
            # NEW v6.0: Pseudoscience
            pseudoscience = dimensions.get('pseudoscience', {})
            if pseudoscience.get('score', 0) > 20:
                patterns.append({
                    'type': 'pseudoscience',
                    'description': f"Pseudoscience indicators: {pseudoscience.get('count', 0)} found",
                    'strength': pseudoscience.get('score', 0)
                })
            
            return patterns
        except Exception as e:
            logger.warning(f"Pattern summary failed: {e}")
            return []
    
    def _generate_objectivity_findings(self, dimensions: Dict[str, Any], objectivity_score: int, 
                                      outlet_name: Optional[str]) -> List[Dict[str, Any]]:
        """Generate findings based on OBJECTIVITY analysis with outlet context"""
        findings = []
        
        try:
            # Overall objectivity assessment with outlet context
            outlet_context = f" from {outlet_name}" if outlet_name else ""
            baseline = dimensions.get('outlet_baseline', {})
            
            if objectivity_score >= 85:
                findings.append({
                    'type': 'positive',
                    'severity': 'positive',
                    'text': f'High objectivity detected ({objectivity_score}/100){outlet_context}',
                    'explanation': 'Article maintains excellent objectivity with minimal bias'
                })
            elif objectivity_score >= 70:
                findings.append({
                    'type': 'positive',
                    'severity': 'positive',
                    'text': f'Good objectivity ({objectivity_score}/100){outlet_context}',
                    'explanation': 'Article shows good objectivity with minor bias elements'
                })
            elif objectivity_score >= 50:
                findings.append({
                    'type': 'warning',
                    'severity': 'medium',
                    'text': f'Moderate objectivity ({objectivity_score}/100){outlet_context}',
                    'explanation': 'Article shows some bias that readers should be aware of'
                })
            else:
                findings.append({
                    'type': 'warning',
                    'severity': 'high',
                    'text': f'Low objectivity ({objectivity_score}/100){outlet_context}',
                    'explanation': 'Article shows significant bias that may affect interpretation'
                })
            
            # Outlet baseline context
            if baseline.get('known_outlet') and baseline.get('bias_amount', 0) > 20:
                findings.append({
                    'type': 'info',
                    'severity': 'low',
                    'text': f'{outlet_name} typically has {baseline["bias_direction"]} bias',
                    'explanation': f'This outlet is known for {baseline["bias_direction"]} political perspective'
                })
            
            # Political bias
            political = dimensions.get('political', {})
            if political.get('score', 0) > 30:
                findings.append({
                    'type': 'info',
                    'severity': 'medium',
                    'text': f'Political lean: {political.get("label", "Unknown")}',
                    'explanation': f'Article shows {political.get("label", "unknown").lower()} political perspective'
                })
            
            # Sensationalism
            sensationalism = dimensions.get('sensationalism', {})
            if sensationalism.get('score', 0) > 50:
                findings.append({
                    'type': 'warning',
                    'severity': 'medium',
                    'text': f'High sensationalism detected',
                    'explanation': 'Article uses sensational language that may exaggerate issues'
                })
            
            # Loaded language
            loaded = dimensions.get('loaded_language', {})
            if loaded.get('count', 0) > 5:
                findings.append({
                    'type': 'warning',
                    'severity': 'medium',
                    'text': f'Loaded language detected ({loaded.get("count", 0)} instances)',
                    'explanation': 'Article uses biased or emotionally charged language'
                })
            
            # NEW v6.0: Controversial figures
            controversial = dimensions.get('controversial_figures', {})
            if controversial.get('count', 0) > 0:
                figures_list = ', '.join([f['name'] for f in controversial.get('found', [])[:3]])
                findings.append({
                    'type': 'warning',
                    'severity': 'high',
                    'text': f'Controversial figures mentioned: {figures_list}',
                    'explanation': 'Article references figures associated with misinformation'
                })
            
            # NEW v6.0: Pseudoscience
            pseudoscience = dimensions.get('pseudoscience', {})
            if pseudoscience.get('score', 0) > 20:
                findings.append({
                    'type': 'warning',
                    'severity': 'high',
                    'text': f'Pseudoscience indicators detected ({pseudoscience.get("count", 0)})',
                    'explanation': 'Article contains unscientific claims or conspiracy theories'
                })
        
        except Exception as e:
            logger.warning(f"Findings generation failed: {e}")
            findings.append({
                'type': 'info',
                'severity': 'medium',
                'text': 'Objectivity analysis completed with some limitations',
                'explanation': 'Full analysis could not be completed but basic assessment was performed'
            })
        
        return findings
    
    def _generate_objectivity_summary(self, dimensions: Dict[str, Any], objectivity_score: int, 
                                     level: str, outlet_name: Optional[str]) -> str:
        """Generate summary of OBJECTIVITY analysis with outlet context"""
        try:
            outlet_context = f" from {outlet_name}" if outlet_name else ""
            
            if objectivity_score >= 85:
                base = f"This article{outlet_context} maintains excellent objectivity with minimal bias."
            elif objectivity_score >= 70:
                base = f"This article{outlet_context} shows good objectivity with only minor bias elements."
            elif objectivity_score >= 50:
                base = f"This article{outlet_context} shows moderate objectivity with some bias present."
            else:
                base = f"This article{outlet_context} shows limited objectivity with significant bias elements."
            
            # Add specific findings
            dominant = self._get_dominant_bias(dimensions)
            if dominant != 'None' and dominant != 'Unknown':
                base += f" Primary concern: {dominant}."
            
            # Add outlet context if known
            baseline = dimensions.get('outlet_baseline', {})
            if baseline.get('known_outlet'):
                base += f" {outlet_name} typically shows {baseline['bias_direction']} bias."
            
            base += f" Objectivity score: {objectivity_score}%."
            
            return base
        
        except Exception as e:
            logger.warning(f"Summary generation failed: {e}")
            return f"Objectivity analysis completed with {objectivity_score}% objectivity score. See detailed findings for more information."
    
    def get_service_info(self) -> Dict[str, Any]:
        """Get service information"""
        info = super().get_service_info()
        info.update({
            'capabilities': [
                'Outlet-aware bias detection',
                'Multi-dimensional objectivity analysis',
                'Political spectrum detection',
                'Sensationalism measurement',
                'Corporate bias detection',
                'Loaded language extraction',
                'Framing analysis',
                'Controversial figure detection',
                'Pseudoscience indicator detection',
                'Source diversity assessment',
                'AI-enhanced bias detection' if self._ai_available else 'Pattern-based bias detection',
                'Timeout-protected analysis',
                'Objectivity scoring (higher is better)'
            ],
            'dimensions_analyzed': 7,
            'outlets_tracked': len(self.outlet_baselines),
            'controversial_figures_tracked': len(self.controversial_figures),
            'patterns_detected': len(self.loaded_patterns),
            'visualization_ready': True,
            'ai_enhanced': self._ai_available,
            'timeout_protected': True,
            'scoring_type': 'objectivity',
            'version': '6.0.0'
        })
        return info
