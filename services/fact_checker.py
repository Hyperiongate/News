"""

CRITICAL FIX IN v13.2:
âœ… FIXED: Claim extraction now properly identifies news article claims
âœ… FIXED: Scoring patterns recognize construction/renovation news
âœ… FIXED: Lowered penalties for "may/could" in news context
âœ… FIXED: Better recognition of official statements and plans
âœ… FIXED: Historical facts and dates now scored properly
âœ… PRESERVED: All v13.1 multi-source aggregation features
âœ… PRESERVED: All 13-point grading scale verdicts
âœ… PRESERVED: All parallel checking optimizations

CHANGES FROM v13.1:
- _score_claim_likelihood_enhanced(): Enhanced scoring for news articles
- Added patterns for: construction/renovation, cost estimates, official plans
- Reduced penalty for "may/reportedly" in official contexts
- Added bonus for proper nouns + specific facts
- Better historical date recognition (e.g., "in 1942")
- Lowered threshold from 10 to 8

Fact Checker Service - v13.2 CLAIM EXTRACTION FIXED FOR NEWS ARTICLES
Fact Checker Service - v13.1 SCORING FIX
Last Updated: October 23, 2025 - FIXED CLAIM EXTRACTION FOR NEWS CONTENT
Last Updated: October 23, 2025 - FIXED SCORING LOGIC
MAJOR ENHANCEMENTS IN v13.1:

CRITICAL FIX IN v13.1:
âœ… FIXED: Unverified claims now properly reduce the score
âœ… FIXED: 100/100 now reserved for articles where ALL claims are TRUE
âœ… FIXED: Proper weighted scoring that includes ALL claims

âœ… NEW: Cross-source verification aggregation
âœ… NEW: Combined verdict analysis (verdict + contextual notes)
âœ… NEW: Multi-outlet corroboration tracking
âœ… NEW: Transparent reasoning chain showing how conclusion was reached
âœ… NEW: Evidence synthesis from multiple verification methods
âœ… IMPROVED: Better use of available resources instead of "needs context"
âœ… IMPROVED: Contextual notes added to verdicts

PRESERVED FROM v12.3:
âœ… All 13-point grading scale verdicts
âœ… All claim extraction and deduplication
âœ… All parallel checking optimizations
âœ… All AI verification features
âœ… Donor/contribution claim detection
âœ… All scoring patterns

KEY PHILOSOPHY CHANGES:
1. Don't say "can't see original" â†’ Say "multiple outlets corroborate this"
2. Don't pick one method â†’ Aggregate all available verification
3. Don't give simple verdict â†’ Provide verdict + context notes
4. Don't hide reasoning â†’ Show exactly how we reached the conclusion

Save as: services/fact_checker.py (REPLACE existing file)
"""

import re
import json
import time
import hashlib
import logging
import requests
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from urllib.parse import quote
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError
from collections import Counter, defaultdict

try:
    from openai import OpenAI
    import httpx
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logging.warning("OpenAI library not available for FactChecker")

from services.base_analyzer import BaseAnalyzer
from config import Config

logger = logging.getLogger(__name__)


# v13.0: 13-Point Verdict Type Definitions (PRESERVED)
VERDICT_TYPES = {
    'true': {
        'label': 'True',
        'icon': 'âœ…',
        'color': '#10b981',
        'score': 100,
        'description': 'Demonstrably accurate and supported by evidence'
    },
    'mostly_true': {
        'label': 'Mostly True',
        'icon': 'âœ“',
        'color': '#34d399',
        'score': 85,
        'description': 'Largely accurate with minor imprecision'
    },
    'partially_true': {
        'label': 'Partially True',
        'icon': 'âš ï¸',
        'color': '#fbbf24',
        'score': 65,
        'description': 'Contains both accurate and inaccurate elements'
    },
    'exaggerated': {
        'label': 'Exaggerated',
        'icon': 'ðŸ“ˆ',
        'color': '#f59e0b',
        'score': 50,
        'description': 'Based on truth but significantly overstated'
    },
    'misleading': {
        'label': 'Misleading',
        'icon': 'âš ï¸',
        'color': '#f97316',
        'score': 35,
        'description': 'Contains truth but creates false impression'
    },
    'mostly_false': {
        'label': 'Mostly False',
        'icon': 'âŒ',
        'color': '#f87171',
        'score': 20,
        'description': 'Significant inaccuracies with grain of truth'
    },
    'false': {
        'label': 'False',
        'icon': 'âŒ',
        'color': '#ef4444',
        'score': 0,
        'description': 'Demonstrably incorrect'
    },
    'empty_rhetoric': {
        'label': 'Empty Rhetoric',
        'icon': 'ðŸ’¨',
        'color': '#94a3b8',
        'score': 50,  # CHANGED: Was None, now 50 (neutral)
        'description': 'Vague promises or boasts with no substantive content'
    },
    'unsubstantiated_prediction': {
        'label': 'Unsubstantiated Prediction',
        'icon': 'ðŸ”®',
        'color': '#a78bfa',
        'score': 50,  # CHANGED: Was None, now 50 (neutral)
        'description': 'Future claim with no evidence or plan provided'
    },
    'needs_context': {
        'label': 'Needs Context',
        'icon': 'â“',
        'color': '#8b5cf6',
        'score': 45,  # CHANGED: Was None, now 45 (slight penalty)
        'description': 'Cannot verify without additional information'
    },
    'opinion': {
        'label': 'Opinion',
        'icon': 'ðŸ’­',
        'color': '#6366f1',
        'score': 50,  # CHANGED: Was None, now 50 (neutral)
        'description': 'Subjective claim analyzed for factual elements'
    },
    'mixed': {
        'label': 'Mixed',
        'icon': 'â—',
        'color': '#f59e0b',
        'score': 50,
        'description': 'Both accurate and inaccurate elements present'
    },
    'unverified': {
        'label': 'Unverified',
        'icon': '?',
        'color': '#9ca3af',
        'score': 40,  # CHANGED: Was 50, now 40 (penalty for lack of verification)
        'description': 'Cannot verify with available information'
    }
}


class FactChecker(BaseAnalyzer):
    """
    ENHANCED Fact-checker with MULTI-SOURCE AGGREGATION
    v13.0 - Cross-verification and transparent reasoning
    """
    
    def __init__(self):
        super().__init__('fact_checker')
        
        # Initialize OpenAI with 5s timeout
        self.openai_client = None
        if OPENAI_AVAILABLE and Config.OPENAI_API_KEY:
            try:
                self.openai_client = OpenAI(
                    api_key=Config.OPENAI_API_KEY,
                    timeout=httpx.Timeout(5.0, connect=2.0)
                )
                logger.info("[FactChecker v13.2] OpenAI client initialized")
            except Exception as e:
                logger.warning(f"[FactChecker v13.2] Failed to initialize OpenAI: {e}")
                self.openai_client = None
        
        # ThreadPoolExecutor for parallel checking
        self.executor = ThreadPoolExecutor(max_workers=10)
        
        # Cache for fact check results
        self.cache = {}
        self.cache_ttl = 86400
        
        # API configuration
        self.google_api_key = Config.GOOGLE_FACT_CHECK_API_KEY or Config.GOOGLE_FACTCHECK_API_KEY
        
        # Initialize patterns
        self.claim_patterns = self._initialize_claim_patterns()
        self.exclusion_patterns = self._initialize_exclusion_patterns()
        
        # Current context
        self.current_date = datetime.now().strftime("%B %d, %Y")
        self.current_year = datetime.now().year
        self.current_us_president = "Donald Trump"
        
        # Verdict types
        self.verdict_types = VERDICT_TYPES
        
        logger.info(f"[FactChecker v13.2] MULTI-SOURCE AGGREGATION ENABLED")
        logger.info(f"[FactChecker v13.2] Context: {self.current_date}, President: {self.current_us_president}")
        logger.info(f"[FactChecker v13.2] 13-POINT SCALE + Cross-verification")
    
    def _check_availability(self) -> bool:
        """Service is always available"""
        return True
    
    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze article with v13.0 multi-source verification
        """
        try:
            start_time = time.time()
            
            # Extract content
            content = data.get('text', '') or data.get('content', '')
            if not content:
                return self.get_error_result("No content provided for fact checking")
            
            # Extract metadata
            article_url = data.get('url', '')
            article_title = data.get('title', '')
            article_date = data.get('publish_date', '')
            sources_count = data.get('sources_count', 0)
            quotes_count = data.get('quotes_count', 0)
            author = data.get('author', '')
            
            logger.info(f"[FactChecker v13.2] Analyzing: {len(content)} chars, {sources_count} sources")
            
            # 1. Extract claims with deduplication (PRESERVED from v12.3)
            extracted_claims = self._extract_claims_enhanced(content)
            logger.info(f"[FactChecker v13.2] Extracted {len(extracted_claims)} UNIQUE claims")
            
            # 2. Check claims in parallel with v13.0 multi-source aggregation
            fact_checks = self._check_claims_parallel(extracted_claims, article_url, article_title)
            
            # 3. Enhanced scoring with 13-point scale (PRESERVED)
            verification_score = self._calculate_score_with_13point_scale(
                fact_checks, sources_count, quotes_count, len(extracted_claims), bool(author)
            )
            
            verification_level = self._get_verification_level(verification_score)
            
            # 4. Generate detailed findings
            findings = self._generate_detailed_findings(fact_checks, sources_count, verification_score)
            
            # 5. Generate comprehensive analysis
            analysis = self._generate_comprehensive_analysis(
                fact_checks, verification_score, sources_count, quotes_count, len(extracted_claims)
            )
            
            # 6. Generate conversational summary
            summary = self._generate_conversational_summary(fact_checks, verification_score, sources_count)
            
            # 7. Identify sources used
            sources_used = self._get_sources_used(fact_checks)
            
            # Count verdicts using 13-point scale
            verdict_counts = self._count_verdicts_by_type(fact_checks)
            
            # Build comprehensive result
            result = {
                'service': self.service_name,
                'success': True,
                'available': True,
                'timestamp': time.time(),
                'analysis_complete': True,
                
                # Core scores
                'score': verification_score,
                'level': verification_level,
                'verification_score': verification_score,
                'verification_level': verification_level,
                'accuracy_score': verification_score,
                
                # Detailed findings
                'findings': findings,
                'analysis': analysis,
                'summary': summary,
                
                # Claim statistics
                'claims_found': len(extracted_claims),
                'claims_checked': len(fact_checks),
                'verdict_counts': verdict_counts,
                
                # Detailed fact checks (for display) - v13.0 ENHANCED
                'fact_checks': self._enrich_fact_checks_with_metadata(fact_checks[:10]),
                'claims': self._enrich_fact_checks_with_metadata(fact_checks[:10]),
                
                # Sources
                'sources_used': sources_used,
                
                # Chart data
                'chart_data': self._generate_13point_chart_data(verdict_counts),
                
                # Details
                'details': {
                    'total_claims': len(extracted_claims),
                    'verdict_breakdown': verdict_counts,
                    'verification_rate': round(sum(v for k, v in verdict_counts.items() if k != 'unverified') / max(len(fact_checks), 1) * 100, 1),
                    'average_confidence': round(sum(fc.get('confidence', 0) for fc in fact_checks) / max(len(fact_checks), 1), 1),
                    'sources_cited': sources_count,
                    'quotes_included': quotes_count,
                    'has_author': bool(author)
                },
                
                'metadata': {
                    'analysis_time': time.time() - start_time,
                    'text_length': len(content),
                    'article_url': article_url,
                    'article_title': article_title,
                    'version': '13.2.0',
                    'grading_scale': '13-point',
                    'ai_enhanced': bool(self.openai_client),
                    'parallel_checking': True,
                    'multi_source_aggregation': True,  # NEW in v13.0
                    'current_date_context': self.current_date,
                    'current_president': self.current_us_president,
                    'claim_extraction': 'FIXED v13.2 - News article patterns enhanced'
                }
            }
            
            logger.info(f"[FactChecker v13.2] Complete: {verification_score}/100 ({verification_level})")
            logger.info(f"[FactChecker v13.2] Verdicts: {verdict_counts}")
            return self.get_success_result(result)
            
        except Exception as e:
            logger.error(f"[FactChecker v13.2] Error: {e}", exc_info=True)
            return self.get_error_result(f"Fact checking error: {str(e)}")
    
    # ============================================================================
    # v12.3: ENHANCED CLAIM EXTRACTION (PRESERVED)
    # ============================================================================
    
    def _extract_claims_enhanced(self, content: str) -> List[str]:
        """
        v12.3: Extract unique, valid claims only (PRESERVED)
        """
        sentences = self._split_sentences(content)
        claims = []
        seen_claims = set()
        
        logger.info(f"[FactChecker v13.2] Evaluating {len(sentences)} sentences for claims...")
        
        for i, sentence in enumerate(sentences):
            if self._matches_exclusion_patterns(sentence):
                continue
            
            score = self._score_claim_likelihood_enhanced(sentence)
            
            if score >= 8:  # LOWERED from 10 to 8 to catch more news claims
                claim = sentence.strip()
                
                if 30 < len(claim) < 400:
                    claim_key = claim.lower()[:100]
                    
                    if claim_key not in seen_claims:
                        claims.append(claim)
                        seen_claims.add(claim_key)
                        logger.debug(f"[FactChecker v13.2] Claim {len(claims)}: score={score}, len={len(claim)}")
                    else:
                        logger.debug(f"[FactChecker v13.2] SKIPPED duplicate claim: {claim[:50]}...")
        
        final_claims = claims[:10]
        
        logger.info(f"[FactChecker v13.2] Extracted {len(final_claims)} unique, validated claims")
        
        return final_claims
    
    def _score_claim_likelihood_enhanced(self, sentence: str) -> int:
        """
        v12.3: Better scoring with mandatory factual elements (PRESERVED)
        """
        score = 0
        sentence_lower = sentence.lower()
        
        
        
        # v13.2: NEW - More lenient base requirements
        has_number = bool(re.search(r'\b\d+', sentence))
        has_named_entity = bool(re.search(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+\b', sentence))
        has_specific_noun = bool(re.search(r'\b(?:building|facility|project|plan|renovation|construction|program|policy|law|bill)\b', sentence_lower))
        has_donor_pattern = bool(re.search(r'\b(?:donat|contribut|fund|sponsor)\w*\b', sentence_lower))
        
        # v13.2: FIXED - Accept if has ANY factual element
        if not (has_number or has_named_entity or has_specific_noun or has_donor_pattern):
            return 0
        
        # Research/studies (highest confidence)
        if re.search(r'\b(?:study|research|report|survey|poll|analysis)\s+(?:shows?|finds?|found|indicates?|suggests?|reveals?)\b', sentence_lower):
            score += 30
        
        # Statistics and numbers
        if re.search(r'\b\d+\s*(?:percent|%)\b', sentence):
            score += 25
        
        if re.search(r'\b\d+(?:,\d{3})*\s+(?:people|workers|employees|individuals|Americans|voters)\b', sentence):
            score += 25
        
        # Official announcements
        if re.search(r'\b(?:announced?|declared?|stated?|confirmed?|revealed?|disclosed?)\s+(?:that|today|yesterday|this week)\b', sentence_lower):
            score += 20
        
        # Government/administration actions
        if re.search(r'\b(?:administration|government|agency|department|officials?)\s+(?:says?|said|announced?|confirmed?|reported?)\b', sentence_lower):
            score += 20
        
        # Policy/action statements
        if re.search(r'\b(?:will|plans to|intends to|expects to|is expected to)\s+\w+\s+(?:employees?|workers?|people|programs?|funding)\b', sentence_lower):
            score += 18
        
        # Direct quotes
        if '"' in sentence and len(re.findall(r'"[^"]{20,}"', sentence)) > 0:
            score += 15
        
        # Attribution phrases
        if re.search(r'\b(?:according to|as reported by|as stated by|officials? said|sources? (?:say|said|told))\b', sentence_lower):
            score += 18
        
        # Numeric changes
        if re.search(r'\b(?:increased?|decreased?|rose|fell|grew|declined?|dropped?)\s+(?:by|to|from)\s+\d+', sentence_lower):
            score += 12
        
        # v13.2: NEW - Construction/renovation claims
        if re.search(r'\b(?:demolished?|rebuilt?|construct(?:ed|ion)?|renovat(?:ed|ion|ions)?|built?|expand(?:ed|sion)?)\b', sentence_lower):
            score += 15
        
        # v13.2: NEW - Cost/budget estimates
        if re.search(r'\$\s*\d+(?:,\d{3})*(?:\s+(?:million|billion|thousand))?|(?:million|billion|thousand)\s+dollars?', sentence_lower):
            score += 18
        
        # v13.2: NEW - Official plans and proposals
        if re.search(r'\b(?:plan|proposal|project|initiative)\s+(?:is|was|will be|would be|may be|could be)\b', sentence_lower):
            score += 12
        
        
        # Dates and timeframes
        if re.search(r'\b(?:in|by|since|from|during)\s+\d{4}\b', sentence):
            score += 12  # v13.2: Increased from 10
        
        if re.search(r'\b(?:originally|first|initially)\s+(?:built|constructed|established|founded)\s+(?:in\s+)?\d{4}\b', sentence_lower):
            score += 15  # v13.2: NEW - Historical construction dates
        
        # Named entities making statements
        if re.search(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\s+(?:said|told|announced|confirmed|stated)\b', sentence):
            score += 10
        
        # v13.2: NEW - Sources and attributions
        if re.search(r'\baccording to\s+(?:sources?|officials?|reports?|documents?|the\s+\w+)\b', sentence_lower):
            score += 10
        
        # v12.3: Donor/contribution detection patterns
        if re.search(r'\b(?:donated?|contributed?|gave|provided)\s+(?:\$|money|funds?|support|to)\b', sentence_lower):
            score += 22
        
        if re.search(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:donated?|contributed?|gave|provided|funded?)\b', sentence):
            score += 20
        
        if re.search(r'\$\s*\d+(?:,\d{3})*(?:\.\d{2})?\s+(?:to|for|toward|in)\b', sentence):
            score += 18
        
        if re.search(r'\b(?:donors?|contributors?|sponsors?|funders?)(?:\s+\w+)?\s+(?:include|are|were|such as)\b', sentence_lower):
            score += 15
        
        # Specific locations/organizations
        if re.search(r'\b(?:in|at|from)\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sentence):
            score += 8
        
        # Time-specific claims
        if re.search(r'\b(?:this year|last year|in 20\d{2}|by 20\d{2})\b', sentence_lower):
            score += 8
        
        # Comparative statements
        if re.search(r'\b(?:more|less|higher|lower|greater|fewer)\s+than\b', sentence_lower):
            score += 8
        
        
        # v13.2: NEW - Structural/technical specifications
        if re.search(r'\b(?:structural|technical|engineering|architectural|design)\s+(?:issues?|problems?|concerns?|requirements?)\b', sentence_lower):
            score += 10
        
        # PENALTIES
        # v13.2: REDUCED PENALTIES for news context
        # "may/reportedly" in official context is normal journalism, smaller penalty
        if re.search(r'\b(?:may|reportedly)\b', sentence_lower):
            if re.search(r'\b(?:official|government|plan|project|administration)\b', sentence_lower):
                score -= 2  # Small penalty in official context
            else:
                score -= 5  # Normal penalty otherwise
        
        if re.search(r'\b(?:might|could|possibly|perhaps|allegedly)\b', sentence_lower):
            score -= 5
        
        if sentence.strip().endswith('?'):
            score -= 15
        
        if re.search(r'\b(?:I think|I believe|in my opinion|seems like|appears to)\b', sentence_lower):
            score -= 10
        
        if len(sentence) < 40:
            score -= 3  # v13.2: Reduced from 5
        
        return max(0, score)
    
    # ============================================================================
    # v13.0: PARALLEL CLAIM CHECKING (ENHANCED)
    # ============================================================================
    
    def _check_claims_parallel(self, claims: List[str], article_url: Optional[str] = None,
                                article_title: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        v13.0: Parallel checking with multi-source aggregation
        """
        if not claims:
            return []
        
        logger.info(f"[FactChecker v13.2] Checking {len(claims)} claims in parallel...")
        
        futures = {}
        for i, claim in enumerate(claims):
            future = self.executor.submit(
                self._verify_single_claim,
                claim, i, article_url, article_title
            )
            futures[future] = (i, claim)
        
        completed_results = []
        for future in as_completed(futures, timeout=15):
            try:
                i, claim = futures[future]
                result = future.result(timeout=1)
                completed_results.append((i, result))
                logger.info(f"[FactChecker v13.2] Claim {i+1}: {result.get('verdict')} ({result.get('confidence')}%)")
            except Exception as e:
                i, claim = futures[future]
                logger.error(f"[FactChecker v13.2] Claim {i+1} failed: {e}")
                completed_results.append((i, {
                    'claim': claim,
                    'verdict': 'unverified',
                    'explanation': 'Verification timeout',
                    'confidence': 0,
                    'sources': [],
                    'method_used': 'timeout'
                }))
        
        completed_results.sort(key=lambda x: x[0])
        fact_checks = [result for _, result in completed_results]
        
        logger.info(f"[FactChecker v13.2] âœ“ Parallel checking complete: {len(fact_checks)} claims")
        return fact_checks
    
    def _verify_single_claim(self, claim: str, index: int,
                             article_url: Optional[str],
                             article_title: Optional[str]) -> Dict[str, Any]:
        """Verify a SINGLE claim with caching"""
        
        try:
            if self.cache:
                cache_key = self._get_cache_key(claim)
                cached_result = self._get_cached_result(cache_key)
                if cached_result:
                    cached_result['from_cache'] = True
                    return cached_result
            
            result = self._verify_claim_comprehensive(claim, index, article_url, article_title)
            
            if self.cache is not None:
                cache_key = self._get_cache_key(claim)
                self._cache_result(cache_key, result)
            
            return result
            
        except Exception as e:
            logger.error(f"[FactChecker v13.2] Error verifying claim {index}: {e}")
            return {
                'claim': claim,
                'verdict': 'unverified',
                'explanation': f'Error: {str(e)}',
                'confidence': 0,
                'sources': [],
                'method_used': 'error'
            }
    
    def _verify_claim_comprehensive(self, claim: str, index: int,
                                   article_url: Optional[str],
                                   article_title: Optional[str]) -> Dict[str, Any]:
        """
        v13.0: ENHANCED - Aggregate results from ALL available methods
        """
        
        try:
            # Skip trivial claims
            if len(claim.strip()) < 20:
                return {
                    'claim': claim,
                    'verdict': 'opinion',
                    'explanation': 'Statement too short to verify meaningfully',
                    'confidence': 50,
                    'sources': [],
                    'evidence': [],
                    'method_used': 'filtered'
                }
            
            # v13.0: NEW APPROACH - Collect results from ALL methods
            verification_results = []
            
            # METHOD 1: AI Analysis
            if self.openai_client:
                ai_result = self._ai_verify_claim(claim, article_title)
                if ai_result:
                    verification_results.append({
                        'method': 'AI Verification',
                        'verdict': ai_result['verdict'],
                        'confidence': ai_result['confidence'],
                        'explanation': ai_result['explanation'],
                        'sources': ai_result.get('sources', []),
                        'weight': 0.4  # AI gets 40% weight
                    })
            
            # METHOD 2: Google Fact Check API
            if self.google_api_key:
                google_result = self._check_google_api(claim)
                if google_result.get('found'):
                    data = google_result['data']
                    verification_results.append({
                        'method': 'Google Fact Check Database',
                        'verdict': data['verdict'],
                        'confidence': data['confidence'],
                        'explanation': data['explanation'],
                        'sources': data.get('sources', []),
                        'fact_check_urls': data.get('fact_check_urls', []),
                        'weight': 0.5  # External fact-checkers get 50% weight
                    })
            
            # METHOD 3: Pattern Analysis (always runs as fallback)
            pattern_result = self._analyze_claim_patterns(claim)
            verification_results.append({
                'method': 'Pattern Analysis',
                'verdict': pattern_result['verdict'],
                'confidence': pattern_result['confidence'],
                'explanation': pattern_result['explanation'],
                'sources': pattern_result.get('sources', []),
                'weight': 0.1  # Patterns get 10% weight
            })
            
            # v13.0: Synthesize all results into single conclusion
            final_result = self._synthesize_verifications(claim, verification_results)
            
            return final_result
            
        except Exception as e:
            logger.error(f"[FactChecker v13.2] Error verifying claim: {e}")
            return {
                'claim': claim,
                'verdict': 'unverified',
                'explanation': f'Verification error: {str(e)}',
                'confidence': 0,
                'sources': [],
                'evidence': [],
                'method_used': 'error'
            }
    
    # ============================================================================
    # v13.0: NEW - MULTI-SOURCE AGGREGATION FUNCTIONS
    # ============================================================================
    
    def _synthesize_verifications(self, claim: str, results: List[Dict]) -> Dict[str, Any]:
        """
        v13.0: NEW - Synthesize multiple verification results into one conclusion
        """
        if not results:
            return {
                'claim': claim,
                'verdict': 'unverified',
                'confidence': 30,
                'explanation': 'No verification methods available',
                'sources': [],
                'method_used': 'none'
            }
        
        # Count verdicts and calculate weighted scores
        verdict_votes = Counter()
        weighted_verdicts = defaultdict(float)
        confidence_sum = 0
        
        for result in results:
            verdict = result['verdict']
            weight = result.get('weight', 1.0)
            confidence = result.get('confidence', 50)
            
            verdict_votes[verdict] += 1
            weighted_verdicts[verdict] += (confidence * weight)
            confidence_sum += confidence
        
        # Determine primary verdict (highest weighted score)
        primary_verdict = max(weighted_verdicts.items(), key=lambda x: x[1])[0]
        
        # Collect all sources from results that agree with primary verdict
        corroborating_sources = []
        for r in results:
            if r.get('sources') and r['verdict'] == primary_verdict:
                corroborating_sources.extend(r['sources'])
        
        unique_sources = list(set(corroborating_sources))
        
        # Build explanation showing the reasoning
        reasoning_chain = self._build_reasoning_chain(results, primary_verdict)
        
        # Extract contextual notes (warnings even if verdict is positive)
        contextual_notes = self._extract_contextual_notes(results, primary_verdict)
        
        # Calculate final confidence
        avg_confidence = confidence_sum / len(results) if results else 50
        
        # Boost confidence if multiple sources agree
        if len(results) >= 2 and verdict_votes[primary_verdict] >= 2:
            avg_confidence = min(avg_confidence + 15, 95)
        
        # Build final result with v13.0 enhancements
        return {
            'claim': claim,
            'verdict': primary_verdict,
            'confidence': int(avg_confidence),
            'explanation': reasoning_chain,
            'contextual_notes': contextual_notes,  # NEW in v13.0
            'sources': unique_sources,
            'corroboration_count': len(unique_sources),  # NEW in v13.0
            'verification_methods': [r['method'] for r in results],
            'method_details': results,  # Full details for transparency
            'cross_verified': len(results) >= 2 and verdict_votes[primary_verdict] >= 2,
            'method_used': ', '.join([r['method'] for r in results])  # For compatibility
        }
    
    def _build_reasoning_chain(self, results: List[Dict], final_verdict: str) -> str:
        """
        v13.0: NEW - Build transparent explanation of how we reached the verdict
        """
        chain_parts = []
        
        # Start with what we checked
        methods = [r['method'] for r in results]
        chain_parts.append(f"We verified this claim using {len(methods)} method(s): {', '.join(methods)}.")
        
        # Detail each method's finding
        for result in results:
            method = result['method']
            verdict = result['verdict']
            confidence = result.get('confidence', 50)
            
            verdict_label = self.verdict_types.get(verdict, {}).get('label', verdict)
            
            if method == 'Google Fact Check Database' and result.get('sources'):
                sources = result['sources'][:3]  # First 3
                if len(sources) > 0:
                    chain_parts.append(
                        f"{method} found this claim rated as '{verdict_label}' "
                        f"by {len(sources)} fact-checker(s): {', '.join(sources)}."
                    )
                else:
                    chain_parts.append(
                        f"{method} assessed this as '{verdict_label}' with {confidence}% confidence."
                    )
            elif method == 'AI Verification':
                chain_parts.append(
                    f"{method} assessed this as '{verdict_label}' with {confidence}% confidence."
                )
            else:
                chain_parts.append(
                    f"{method} identified this as '{verdict_label}'."
                )
        
        # Explain synthesis
        if len(results) >= 2:
            verdict_label = self.verdict_types.get(final_verdict, {}).get('label', final_verdict)
            chain_parts.append(
                f"Based on cross-verification from {len(results)} sources, "
                f"we rate this claim as '{verdict_label}'."
            )
        
        return " ".join(chain_parts)
    
    def _extract_contextual_notes(self, results: List[Dict], primary_verdict: str) -> List[str]:
        """
        v13.0: NEW - Extract important context even if not the primary verdict
        
        Example: Claim is "mostly_true" but one method noted "misleading"
        Result: ['âš ï¸ Potentially misleading framing noted by AI analysis']
        """
        notes = []
        
        # Warning verdicts and concern verdicts
        warning_verdicts = ['misleading', 'exaggerated', 'partially_true']
        concern_verdicts = ['mostly_false', 'false']
        
        for result in results:
            verdict = result['verdict']
            method = result['method']
            explanation = result.get('explanation', '')
            
            # If primary verdict is positive but method found concerns
            if primary_verdict in ['true', 'mostly_true'] and verdict in warning_verdicts:
                notes.append(
                    f"âš ï¸ {method} noted: {verdict.replace('_', ' ').title()}"
                )
            
            # If primary verdict is negative but method found some truth
            if primary_verdict in concern_verdicts and verdict in ['partially_true', 'mostly_true']:
                notes.append(
                    f"â„¹ï¸ {method} found: {verdict.replace('_', ' ').title()}"
                )
            
            # Look for specific keywords in explanations
            if 'misleading' in explanation.lower() and primary_verdict not in ['misleading', 'false']:
                notes.append(f"âš ï¸ Potentially misleading framing noted by {method}")
            
            if 'exaggerat' in explanation.lower() and primary_verdict not in ['exaggerated', 'false']:
                notes.append(f"âš ï¸ Some exaggeration noted by {method}")
            
            if 'context' in explanation.lower() and 'missing' in explanation.lower():
                notes.append(f"â„¹ï¸ Additional context recommended by {method}")
        
        return list(set(notes))  # Remove duplicates
    
    # ============================================================================
    # AI VERIFICATION (PRESERVED from v12.3)
    # ============================================================================
    
    def _ai_verify_claim(self, claim: str, article_context: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """Use AI to verify claim with 13-point scale"""
        if not self.openai_client:
            return None
        
        try:
            prompt = self._build_verification_prompt_13point(claim, article_context)
            
            response = self.openai_client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[
                    {"role": "system", "content": self._get_system_prompt_13point()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=300
            )
            
            result = self._parse_ai_verification_13point(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logger.warning(f"[FactChecker v13.2] AI verification failed: {e}")
            return None
    
    def _get_system_prompt_13point(self) -> str:
        """System prompt with 13-point scale"""
        return f"""You are an expert fact-checker analyzing claims as of {self.current_date}.

CRITICAL CURRENT CONTEXT:
- Today's date: {self.current_date}
- Current year: {self.current_year}
- Current US President: {self.current_us_president}

13-POINT VERDICT SCALE:
1. true - Demonstrably accurate
2. mostly_true - Largely accurate with minor issues
3. partially_true - Mix of true and false elements
4. exaggerated - Based on truth but significantly overstated
5. misleading - Contains truth but creates false impression
6. mostly_false - Significant inaccuracies
7. false - Demonstrably incorrect
8. empty_rhetoric - Vague promises with no substance
9. unsubstantiated_prediction - Future claim with no evidence
10. needs_context - Cannot verify without additional information
11. opinion - Subjective claim
12. mixed - Both accurate and inaccurate elements
13. unverified - Cannot verify with available information

Use information current as of {self.current_date}."""
    
    def _build_verification_prompt_13point(self, claim: str, context: Optional[str] = None) -> str:
        """Build prompt with 13-point scale"""
        prompt_parts = [
            f"Verify this claim using the 13-point scale (as of {self.current_date}):",
            f'"{claim}"',
            ""
        ]
        
        if context:
            prompt_parts.append(f"Context: From article \"{context}\"")
            prompt_parts.append("")
        
        prompt_parts.extend([
            f"IMPORTANT: Today is {self.current_date}. {self.current_us_president} is the current US President.",
            "",
            "Analyze and provide:",
            "1. VERDICT: One of [true, mostly_true, partially_true, exaggerated, misleading, mostly_false, false, empty_rhetoric, unsubstantiated_prediction, needs_context, opinion, mixed, unverified]",
            "2. CONFIDENCE: 50-95%",
            "3. EXPLANATION: Why (2-3 sentences)",
            "",
            "Format EXACTLY:",
            "VERDICT: [verdict from 13-point scale]",
            "CONFIDENCE: [number]",
            "EXPLANATION: [explanation]"
        ])
        
        return "\n".join(prompt_parts)
    
    def _parse_ai_verification_13point(self, response: str) -> Optional[Dict[str, Any]]:
        """Parse AI response with 13-point verdicts"""
        
        try:
            lines = response.strip().split('\n')
            result = {
                'verdict': 'unverified',
                'confidence': 50,
                'explanation': 'AI analysis completed',
                'sources': ['AI Analysis']
            }
            
            for line in lines:
                line = line.strip()
                if line.startswith('VERDICT:'):
                    verdict = line.replace('VERDICT:', '').strip().lower().replace(' ', '_')
                    if verdict in self.verdict_types:
                        result['verdict'] = verdict
                    else:
                        verdict_map = {
                            'accurate': 'true',
                            'correct': 'true',
                            'inaccurate': 'false',
                            'incorrect': 'false',
                            'partly_true': 'partially_true',
                            'half_true': 'partially_true',
                            'rhetoric': 'empty_rhetoric',
                            'prediction': 'unsubstantiated_prediction',
                            'subjective': 'opinion',
                            'context_needed': 'needs_context'
                        }
                        result['verdict'] = verdict_map.get(verdict, 'unverified')
                    
                elif line.startswith('CONFIDENCE:'):
                    conf_str = re.findall(r'\d+', line)
                    if conf_str:
                        result['confidence'] = min(int(conf_str[0]), 95)
                        
                elif line.startswith('EXPLANATION:'):
                    explanation = line.replace('EXPLANATION:', '').strip()
                    if explanation:
                        result['explanation'] = explanation
            
            explanation_lines = [l for l in lines if not l.startswith(('VERDICT:', 'CONFIDENCE:', 'EXPLANATION:')) and l.strip()]
            if explanation_lines and len(result['explanation']) < 50:
                result['explanation'] = ' '.join(explanation_lines[:3])
            
            return result
            
        except Exception as e:
            logger.error(f"[FactChecker v13.2] Failed to parse AI response: {e}")
            return None
    
    # ============================================================================
    # GOOGLE FACT CHECK API (PRESERVED from v12.3)
    # ============================================================================
    
    def _check_google_api(self, claim: str) -> Dict[str, Any]:
        """Check Google Fact Check API"""
        if not self.google_api_key:
            return {'found': False}
        
        try:
            url = 'https://factchecktools.googleapis.com/v1alpha1/claims:search'
            params = {
                'key': self.google_api_key,
                'query': claim[:500],
                'languageCode': 'en'
            }
            
            response = requests.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if 'claims' in data and len(data['claims']) > 0:
                    verdicts = []
                    explanations = []
                    publishers = []
                    urls = []
                    
                    for claim_item in data['claims'][:3]:
                        for review in claim_item.get('claimReview', [])[:2]:
                            if 'textualRating' in review:
                                verdicts.append(review['textualRating'])
                            if 'title' in review:
                                explanations.append(review['title'])
                            if 'publisher' in review:
                                publishers.append(review['publisher'].get('name', 'Unknown'))
                            if 'url' in review:
                                urls.append(review['url'])
                    
                    verdict = self._map_google_verdict_to_13point(verdicts)
                    confidence = self._calculate_api_confidence(verdicts, publishers)
                    
                    return {
                        'found': True,
                        'data': {
                            'verdict': verdict,
                            'explanation': explanations[0] if explanations else 'Verified by fact checkers',
                            'confidence': confidence,
                            'sources': list(set(publishers))[:3],
                            'evidence': explanations[:3],
                            'fact_check_urls': urls[:3]
                        }
                    }
            
            return {'found': False}
            
        except Exception as e:
            logger.error(f"[FactChecker v13.2] Google API error: {e}")
            return {'found': False}
    
    def _map_google_verdict_to_13point(self, verdicts: List[str]) -> str:
        """Map Google API verdicts to 13-point scale"""
        if not verdicts:
            return 'unverified'
        
        normalized = []
        for v in verdicts:
            v_lower = v.lower()
            if 'true' in v_lower and 'false' not in v_lower:
                if 'mostly' in v_lower or 'largely' in v_lower:
                    normalized.append('mostly_true')
                elif 'partially' in v_lower or 'partly' in v_lower:
                    normalized.append('partially_true')
                else:
                    normalized.append('true')
            elif 'false' in v_lower:
                if 'mostly' in v_lower or 'largely' in v_lower:
                    normalized.append('mostly_false')
                elif 'partially' in v_lower:
                    normalized.append('partially_true')
                else:
                    normalized.append('false')
            elif 'misleading' in v_lower:
                normalized.append('misleading')
            elif 'exaggerat' in v_lower:
                normalized.append('exaggerated')
            elif 'mixed' in v_lower:
                normalized.append('mixed')
            elif 'context' in v_lower:
                normalized.append('needs_context')
            else:
                normalized.append('unverified')
        
        from collections import Counter
        counts = Counter(normalized)
        return counts.most_common(1)[0][0]
    
    # ============================================================================
    # PATTERN ANALYSIS (PRESERVED from v12.3)
    # ============================================================================
    
    def _analyze_claim_patterns(self, claim: str) -> Dict[str, Any]:
        """Analyze claim using pattern matching"""
        result = {
            'verdict': 'unverified',
            'explanation': 'This claim could not be verified automatically.',
            'confidence': 30,
            'sources': ['Pattern Analysis'],
            'evidence': []
        }
        
        claim_lower = claim.lower()
        
        # Check for opinion indicators
        if re.search(r'\b(?:I think|I believe|in my opinion|seems like|feels like|appears to be)\b', claim_lower):
            result['verdict'] = 'opinion'
            result['explanation'] = 'This appears to be a subjective opinion.'
            return result
        
        # Check for future predictions
        if re.search(r'\b(?:will|going to|plans to|expects to|intends to)\s+(?:be|become|happen|occur)', claim_lower):
            result['verdict'] = 'unsubstantiated_prediction'
            result['explanation'] = 'This is a prediction about the future.'
            return result
        
        # Check for vague rhetoric
        if re.search(r'\b(?:tremendous|fantastic|amazing|incredible|the best|the worst)\b', claim_lower) and not re.search(r'\b\d+', claim):
            result['verdict'] = 'empty_rhetoric'
            result['explanation'] = 'This contains vague superlatives without specific content.'
            return result
        
        # Attribution present
        if re.search(r'\b(?:said|stated|according to|claimed)\b', claim_lower):
            result['confidence'] = 40
            result['explanation'] = 'This is an attributed statement. Verify by checking the original source.'
        
        # Statistics present
        if re.search(r'\b\d+\s*(?:percent|%)\b', claim) or re.search(r'\b\d+\s+(?:million|billion|thousand)\b', claim):
            result['confidence'] = 35
            result['explanation'] = 'This claim contains statistics. Verify against official data sources.'
        
        # Absolute language
        if re.search(r'\b(?:always|never|all|none|every|no one|everyone)\b', claim_lower):
            result['verdict'] = 'exaggerated'
            result['confidence'] = 40
            result['explanation'] = 'This claim uses absolute language which is often a sign of exaggeration.'
            result['evidence'] = ['Contains absolute language']
        
        return result
    
    # ============================================================================
    # HELPER METHODS (PRESERVED from v12.3)
    # ============================================================================
    
    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences"""
        text = re.sub(r'\n+', '. ', text)
        sentences = re.split(r'[.!?]+(?=\s+[A-Z]|\s*$)', text)
        cleaned = [s.strip() for s in sentences if len(s.strip()) > 15 and not s.isupper()]
        return cleaned
    
    def _matches_exclusion_patterns(self, sentence: str) -> bool:
        """Check if sentence matches exclusion patterns"""
        if len(sentence) < 20 or len(sentence) > 500:
            return True
        
        exclusions = [
            r'^(?:By|Reporting by|Written by|Edited by|Photography by)\s+[A-Z]',
            r'^(?:Photo|Image|Video|Figure|Table)\s*:',
            r'^\d+:\d+',
            r'^(?:Click here|Subscribe|Follow us|Sign up|Read more)',
            r'^(?:Copyright|Â©|All rights reserved)',
            r'^(?:This article|This story|This report)\s+(?:was|is)',
            r'^\[.*?\]$',
            r'^https?://',
            r'@\w+'
        ]
        
        for pattern in exclusions:
            if re.match(pattern, sentence, re.IGNORECASE):
                return True
        return False
    
    def _calculate_api_confidence(self, verdicts: List[str], publishers: List[str]) -> int:
        """Calculate confidence from API results"""
        if not verdicts:
            return 30
        
        confidence = 60
        if len(verdicts) >= 3: confidence += 15
        elif len(verdicts) >= 2: confidence += 10
        
        reputable = ['snopes', 'politifact', 'factcheck.org', 'reuters', 'ap']
        for pub in publishers:
            if any(rep in pub.lower() for rep in reputable):
                confidence += 5
                break
        
        return min(confidence, 95)
    
    # ============================================================================
    # SCORING AND METADATA (PRESERVED from v12.3)
    # ============================================================================
    
    def _count_verdicts_by_type(self, fact_checks: List[Dict[str, Any]]) -> Dict[str, int]:
        """Count verdicts using 13-point scale"""
        counts = {verdict_type: 0 for verdict_type in self.verdict_types.keys()}
        
        for fc in fact_checks:
            verdict = fc.get('verdict', 'unverified')
            if verdict in counts:
                counts[verdict] += 1
            else:
                counts['unverified'] += 1
        
        return counts
    
    def _enrich_fact_checks_with_metadata(self, fact_checks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        v13.0: ENHANCED - Add rich metadata including corroboration info
        """
        enriched = []
        
        for fc in fact_checks:
            verdict = fc.get('verdict', 'unverified')
            verdict_meta = self.verdict_types.get(verdict, self.verdict_types['unverified'])
            
            enriched_fc = fc.copy()
            enriched_fc.update({
                'verdict_label': verdict_meta['label'],
                'verdict_icon': verdict_meta['icon'],
                'verdict_color': verdict_meta['color'],
                'verdict_description': verdict_meta['description'],
                'verdict_score': verdict_meta['score']
            })
            
            # v13.0: NEW - Add corroboration information
            sources_count = fc.get('corroboration_count', 0)
            if sources_count >= 3:
                enriched_fc['corroboration_level'] = 'Strong'
                enriched_fc['corroboration_badge'] = 'âœ“âœ“âœ“'
            elif sources_count >= 2:
                enriched_fc['corroboration_level'] = 'Moderate'
                enriched_fc['corroboration_badge'] = 'âœ“âœ“'
            elif sources_count == 1:
                enriched_fc['corroboration_level'] = 'Single Source'
                enriched_fc['corroboration_badge'] = 'âœ“'
            else:
                enriched_fc['corroboration_level'] = 'Not Corroborated'
                enriched_fc['corroboration_badge'] = ''
            
            # v13.0: NEW - Add verification transparency
            methods = fc.get('verification_methods', [])
            if len(methods) >= 3:
                enriched_fc['verification_strength'] = 'Comprehensive'
            elif len(methods) == 2:
                enriched_fc['verification_strength'] = 'Good'
            else:
                enriched_fc['verification_strength'] = 'Basic'
            
            # v13.0: NEW - Format for frontend display
            if fc.get('cross_verified'):
                enriched_fc['verification_badge'] = 'ðŸ” Cross-Verified'
            
            enriched.append(enriched_fc)
        
        return enriched
    
    def _generate_13point_chart_data(self, verdict_counts: Dict[str, int]) -> Dict[str, Any]:
        """Generate chart data for 13-point scale"""
        labels = []
        data = []
        colors = []
        
        # Group 1: True variants
        true_count = verdict_counts.get('true', 0) + verdict_counts.get('mostly_true', 0)
        if true_count > 0:
            labels.append('True/Mostly True')
            data.append(true_count)
            colors.append('#10b981')
        
        # Group 2: Partially true/Mixed
        partial_count = verdict_counts.get('partially_true', 0) + verdict_counts.get('mixed', 0)
        if partial_count > 0:
            labels.append('Partially True/Mixed')
            data.append(partial_count)
            colors.append('#fbbf24')
        
        # Group 3: Exaggerated/Misleading
        misleading_count = verdict_counts.get('exaggerated', 0) + verdict_counts.get('misleading', 0)
        if misleading_count > 0:
            labels.append('Exaggerated/Misleading')
            data.append(misleading_count)
            colors.append('#f59e0b')
        
        # Group 4: False variants
        false_count = verdict_counts.get('false', 0) + verdict_counts.get('mostly_false', 0)
        if false_count > 0:
            labels.append('False/Mostly False')
            data.append(false_count)
            colors.append('#ef4444')
        
        # Group 5: Non-factual
        non_factual = (verdict_counts.get('opinion', 0) + 
                      verdict_counts.get('empty_rhetoric', 0) + 
                      verdict_counts.get('unsubstantiated_prediction', 0))
        if non_factual > 0:
            labels.append('Non-Factual Claims')
            data.append(non_factual)
            colors.append('#94a3b8')
        
        # Group 6: Unverified/Needs Context
        unverified_count = verdict_counts.get('unverified', 0) + verdict_counts.get('needs_context', 0)
        if unverified_count > 0:
            labels.append('Unverified/Needs Context')
            data.append(unverified_count)
            colors.append('#9ca3af')
        
        return {
            'type': 'doughnut',
            'data': {
                'labels': labels,
                'datasets': [{
                    'data': data,
                    'backgroundColor': colors
                }]
            }
        }
    
    def _calculate_score_with_13point_scale(self, fact_checks: List[Dict], sources_count: int,
                                            quotes_count: int, total_claims: int, has_author: bool) -> int:
        """
        v13.1: FIXED SCORING LOGIC
        Calculate score properly counting ALL claims including unverified
        """
        
        # Base score for having content
        base_score = 20
        
        # Source scoring (up to 25 points)
        source_score = min(25, sources_count * 5)
        
        # Quote scoring (up to 15 points)
        quote_score = min(15, quotes_count * 5)
        
        # ===== CRITICAL FIX: Claim scoring (up to 35 points) =====
        claim_score = 0
        if fact_checks and len(fact_checks) > 0:
            all_verdict_scores = []
            
            for fc in fact_checks:
                verdict = fc.get('verdict', 'unverified')
                verdict_meta = self.verdict_types.get(verdict, self.verdict_types['unverified'])
                
                # FIXED: All verdicts now have scores (no more None values)
                verdict_score = verdict_meta['score']
                all_verdict_scores.append(verdict_score)
            
            if all_verdict_scores:
                # Calculate average of ALL claims
                avg_claim_score = sum(all_verdict_scores) / len(all_verdict_scores)
                # Convert to 0-35 point scale
                claim_score = int((avg_claim_score / 100.0) * 35)
        
        # Author bonus (up to 3 points)
        author_score = 3 if has_author else 0
        
        # Complexity bonus (up to 2 points)
        complexity_score = 2 if total_claims >= 5 else (1 if total_claims >= 3 else 0)
        
        # Calculate final score (max 100)
        final_score = base_score + source_score + quote_score + claim_score + author_score + complexity_score
        final_score = int(max(0, min(100, final_score)))
        
        logger.info(f"[FactChecker v13.2] FIXED SCORE BREAKDOWN:")
        logger.info(f"  Base:       {base_score}/20")
        logger.info(f"  Sources:    {source_score}/25 ({sources_count} sources)")
        logger.info(f"  Quotes:     {quote_score}/15 ({quotes_count} quotes)")
        logger.info(f"  Claims:     {claim_score}/35 ({len(fact_checks)} claims)")
        logger.info(f"  Author:     {author_score}/3")
        logger.info(f"  Complexity: {complexity_score}/2")
        logger.info(f"  FINAL:      {final_score}/100")
        
        return final_score
    
    def _generate_detailed_findings(self, fact_checks: List[Dict[str, Any]], 
                                    sources_count: int, score: int) -> List[Dict[str, Any]]:
        """Generate detailed findings"""
        findings = []
        
        verdict_counts = self._count_verdicts_by_type(fact_checks)
        
        if verdict_counts.get('false', 0) > 0 or verdict_counts.get('mostly_false', 0) > 0:
            false_count = verdict_counts.get('false', 0) + verdict_counts.get('mostly_false', 0)
            false_claims = [fc for fc in fact_checks if fc.get('verdict') in ['false', 'mostly_false']]
            findings.append({
                'type': 'critical',
                'severity': 'high',
                'text': f'{false_count} claim(s) found to be false or mostly false',
                'explanation': f'We identified inaccurate claims. Example: "{false_claims[0].get("claim", "")[:100]}..."',
                'examples': [fc.get('claim', '')[:150] for fc in false_claims[:2]]
            })
        
        if verdict_counts.get('misleading', 0) > 0 or verdict_counts.get('exaggerated', 0) > 0:
            misleading_count = verdict_counts.get('misleading', 0) + verdict_counts.get('exaggerated', 0)
            findings.append({
                'type': 'warning',
                'severity': 'medium',
                'text': f'{misleading_count} claim(s) found to be misleading or exaggerated',
                'explanation': 'These claims contain elements of truth but misrepresent or overstate facts.'
            })
        
        true_count = verdict_counts.get('true', 0) + verdict_counts.get('mostly_true', 0)
        if true_count > 0:
            findings.append({
                'type': 'positive',
                'severity': 'positive',
                'text': f'{true_count} claim(s) verified as accurate',
                'explanation': 'These claims were confirmed through fact-checking databases or AI verification'
            })
        
        unverified = verdict_counts.get('unverified', 0)
        if unverified > len(fact_checks) * 0.5 and len(fact_checks) > 0:
            findings.append({
                'type': 'warning',
                'severity': 'medium',
                'text': f'{unverified} claim(s) could not be verified',
                'explanation': 'Many claims lack available verification. Verify important claims independently.'
            })
        
        if score >= 70:
            findings.append({
                'type': 'positive',
                'severity': 'positive',
                'text': f'Strong verification score ({score}/100)',
                'explanation': 'Article demonstrates good sourcing and factual accuracy'
            })
        elif score < 50:
            findings.append({
                'type': 'warning',
                'severity': 'high',
                'text': f'Low verification score ({score}/100)',
                'explanation': 'Limited sourcing and verification. Treat claims with skepticism.'
            })
        
        if sources_count >= 5:
            findings.append({
                'type': 'positive',
                'severity': 'positive',
                'text': f'Well-sourced ({sources_count} sources cited)',
                'explanation': 'Article provides adequate citations to verify claims'
            })
        elif sources_count == 0:
            findings.append({
                'type': 'warning',
                'severity': 'high',
                'text': 'No sources cited',
                'explanation': 'Article lacks citations, making claims difficult to verify'
            })
        
        return findings
    
    def _generate_comprehensive_analysis(self, fact_checks: List[Dict[str, Any]], 
                                        score: int, sources_count: int,
                                        quotes_count: int, total_claims: int) -> Dict[str, str]:
        """Generate comprehensive analysis"""
        
        verification_methods = []
        if any(fc.get('method_used') and 'AI' in fc.get('method_used', '') for fc in fact_checks):
            verification_methods.append('AI verification with 13-point scale')
        if any(fc.get('method_used') and 'Google' in fc.get('method_used', '') for fc in fact_checks):
            verification_methods.append('Google Fact Check database')
        verification_methods.append('pattern analysis')
        
        what_we_looked = (
            f"We extracted {total_claims} factual claims from the article and verified them using "
            f"{', '.join(verification_methods)}. Each claim was evaluated using our comprehensive "
            f"13-point grading scale. We also analyzed the article's sourcing quality "
            f"({sources_count} sources cited, {quotes_count} quotes) and author attribution."
        )
        
        verdict_counts = self._count_verdicts_by_type(fact_checks)
        
        findings_parts = []
        if verdict_counts.get('true', 0) > 0:
            findings_parts.append(f"{verdict_counts['true']} claim(s) verified as TRUE")
        if verdict_counts.get('mostly_true', 0) > 0:
            findings_parts.append(f"{verdict_counts['mostly_true']} MOSTLY TRUE")
        if verdict_counts.get('partially_true', 0) > 0:
            findings_parts.append(f"{verdict_counts['partially_true']} PARTIALLY TRUE")
        if verdict_counts.get('exaggerated', 0) > 0:
            findings_parts.append(f"{verdict_counts['exaggerated']} EXAGGERATED")
        if verdict_counts.get('misleading', 0) > 0:
            findings_parts.append(f"{verdict_counts['misleading']} MISLEADING")
        if verdict_counts.get('mostly_false', 0) > 0:
            findings_parts.append(f"{verdict_counts['mostly_false']} MOSTLY FALSE")
        if verdict_counts.get('false', 0) > 0:
            findings_parts.append(f"{verdict_counts['false']} FALSE")
        
        what_we_found = ". ".join(findings_parts) + f". The article cites {sources_count} sources."
        
        if score >= 70:
            what_it_means = (
                f"This article demonstrates strong factual accuracy ({score}/100). "
                f"Using our 13-point grading scale, most claims were verified as accurate. "
                f"Readers can generally trust the information presented."
            )
        elif score >= 50:
            what_it_means = (
                f"This article has moderate verification ({score}/100). "
                f"Our 13-point analysis found mixed accuracy. "
                f"Exercise caution and cross-reference important information."
            )
        else:
            what_it_means = (
                f"This article has low verification ({score}/100). "
                f"Our 13-point grading scale identified significant accuracy concerns. "
                f"Verify all important information independently."
            )
        
        return {
            'what_we_looked': what_we_looked,
            'what_we_found': what_we_found,
            'what_it_means': what_it_means
        }
    
    def _generate_conversational_summary(self, fact_checks: List[Dict[str, Any]],
                                         score: int, sources_count: int) -> str:
        """Generate conversational summary"""
        if len(fact_checks) == 0:
            return f"No specific fact-checkable claims found. Article cites {sources_count} sources. Verification score: {score}/100."
        
        verdict_counts = self._count_verdicts_by_type(fact_checks)
        
        summary = f"Checked {len(fact_checks)} claims using 13-point scale. "
        
        true_count = verdict_counts.get('true', 0) + verdict_counts.get('mostly_true', 0)
        false_count = verdict_counts.get('false', 0) + verdict_counts.get('mostly_false', 0)
        
        if true_count > 0 and false_count == 0:
            summary += f"{true_count} verified as accurate. "
        elif true_count > 0 and false_count > 0:
            summary += f"{true_count} accurate, {false_count} false. "
        elif false_count > 0:
            summary += f"{false_count} found to be false or misleading. "
        
        summary += f"Article cites {sources_count} sources. "
        summary += f"Overall verification: {score}/100."
        
        return summary
    
    def _get_verification_level(self, score: int) -> str:
        """Convert score to level"""
        if score >= 80: return 'Highly Verified'
        elif score >= 60: return 'Well Verified'
        elif score >= 40: return 'Partially Verified'
        else: return 'Poorly Verified'
    
    def _get_sources_used(self, fact_checks: List[Dict[str, Any]]) -> List[str]:
        """Get unique sources used"""
        sources = set()
        for fc in fact_checks:
            if 'sources' in fc and isinstance(fc['sources'], list):
                sources.update(fc['sources'])
            if 'method_used' in fc:
                sources.add(fc['method_used'])
        return list(sources)
    
    def _get_cache_key(self, claim: str) -> str:
        """Generate cache key"""
        return hashlib.sha256(claim.encode()).hexdigest()[:16]
    
    def _get_cached_result(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Get cached result"""
        if cache_key in self.cache:
            cached_time, result = self.cache[cache_key]
            if time.time() - cached_time < self.cache_ttl:
                return result.copy()
        return None
    
    def _cache_result(self, cache_key: str, result: Dict[str, Any]):
        """Cache result"""
        self.cache[cache_key] = (time.time(), result.copy())
        if len(self.cache) > 1000:
            sorted_items = sorted(self.cache.items(), key=lambda x: x[1][0])
            for key, _ in sorted_items[:100]:
                del self.cache[key]
    
    def _initialize_claim_patterns(self) -> Dict[str, Any]:
        """Initialize patterns"""
        return {'claim_indicators': []}
    
    def _initialize_exclusion_patterns(self) -> List[str]:
        """Initialize exclusion patterns"""
        return []
    
    def get_service_info(self) -> Dict[str, Any]:
        """Get service information"""
        info = super().get_service_info()
        info.update({
            'version': '13.2.0',
            'grading_scale': '13-point comprehensive scale',
            'optimization': 'Multi-source aggregation with cross-verification',
            'current_context': f'{self.current_date}, President {self.current_us_president}',
            'claim_extraction': 'FIXED v13.2 - News article patterns enhanced',
            'verdict_types': list(self.verdict_types.keys()),
            'capabilities': [
                'FIXED: No more repeated claims',
                'FIXED: No more non-existent claims',
                'FIXED: Better claim quality validation',
                'NEW: Multi-source verification aggregation',
                'NEW: Cross-verification from multiple methods',
                'NEW: Transparent reasoning chains',
                'NEW: Contextual notes for nuanced verdicts',
                'Parallel claim verification (10 workers)',
                'AI-powered with 13-point grading scale',
                'Google Fact Check database integration',
                'Current date and political awareness',
                'Comprehensive verdict types'
            ],
            'verification_methods': [
                'AI Verification (13-point scale)' if self.openai_client else None,
                'Google Fact Check Database' if self.google_api_key else None,
                'Pattern Analysis'
            ],
            'ai_enhanced': bool(self.openai_client),
            'parallel_processing': True,
            'multi_source_aggregation': True
        })
        return info


logger.info("[FactChecker v13.2] Module loaded - CLAIM EXTRACTION FIXED FOR NEWS ARTICLES!")

# This file is not truncated
